{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import cho_factor, cho_solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_unwrap(param_vec, shapes, sizes):\n",
    "    \"\"\"Helper routine for minimize_list\"\"\"\n",
    "    args = []\n",
    "    pos = 0\n",
    "    for i in range(len(shapes)):\n",
    "        sz = sizes[i]\n",
    "        args.append(param_vec[pos:pos+sz].reshape(shapes[i]))\n",
    "        pos += sz\n",
    "    return args\n",
    "\n",
    "\n",
    "def params_wrap(param_list):\n",
    "    \"\"\"Helper routine for minimize_list\"\"\"\n",
    "    param_list = [np.array(x) for x in param_list]\n",
    "    shapes = [x.shape for x in param_list]\n",
    "    sizes = [x.size for x in param_list]\n",
    "    param_vec = np.zeros(sum(sizes))\n",
    "    pos = 0\n",
    "    for param in param_list:\n",
    "        sz = param.size\n",
    "        param_vec[pos:pos+sz] = param.ravel()\n",
    "        pos += sz\n",
    "    unwrap = lambda pvec: params_unwrap(pvec, shapes, sizes)\n",
    "    return param_vec, unwrap\n",
    "\n",
    "\n",
    "def minimize_list(cost, init_list, args):\n",
    "    \"\"\"Optimize a list of arrays (wrapper of scipy.optimize.minimize)\n",
    "\n",
    "    The input function \"cost\" should take a list of parameters,\n",
    "    followed by any extra arguments:\n",
    "        cost(init_list, *args)\n",
    "    should return the cost of the initial condition, and a list in the same\n",
    "    format as init_list giving gradients of the cost wrt the parameters.\n",
    "\n",
    "    The options to the optimizer have been hard-coded. You may wish\n",
    "    to change disp to True to get more diagnostics. You may want to\n",
    "    decrease maxiter while debugging. Although please report all results\n",
    "    in Q2-5 using maxiter=500.\n",
    "    \"\"\"\n",
    "    opt = {'maxiter': 500, 'disp': False}\n",
    "    init, unwrap = params_wrap(init_list)\n",
    "    def wrap_cost(vec, *args):\n",
    "        E, params_bar = cost(unwrap(vec), *args)\n",
    "        vec_bar, _ = params_wrap(params_bar)\n",
    "        return E, vec_bar\n",
    "    res = minimize(wrap_cost, init, args, 'L-BFGS-B', jac=True, options=opt)\n",
    "    return unwrap(res.x)\n",
    "\n",
    "\n",
    "def linreg_cost(params, X, yy, alpha):\n",
    "    \"\"\"Regularized least squares cost function and gradients\n",
    "\n",
    "    Can be optimized with minimize_list -- see fit_linreg_gradopt for a\n",
    "    demonstration.\n",
    "\n",
    "    Inputs:\n",
    "    params: tuple (ww, bb): weights ww (D,), bias bb scalar\n",
    "         X: N,D design matrix of input features\n",
    "        yy: N,  real-valued targets\n",
    "     alpha: regularization constant\n",
    "\n",
    "    Outputs: (E, [ww_bar, bb_bar]), cost and gradients\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb = params\n",
    "\n",
    "    # forward computation of error\n",
    "    ff = np.dot(X, ww) + bb\n",
    "    res = ff - yy\n",
    "    E = np.dot(res, res) + alpha*np.dot(ww, ww)\n",
    "\n",
    "    # reverse computation of gradients\n",
    "    ff_bar = 2*res\n",
    "    bb_bar = np.sum(ff_bar)\n",
    "    ww_bar = np.dot(X.T, ff_bar) + 2*alpha*ww\n",
    "\n",
    "    return E, [ww_bar, bb_bar]\n",
    "\n",
    "\n",
    "def fit_linreg_gradopt(X, yy, alpha):\n",
    "    \"\"\"\n",
    "    fit a regularized linear regression model with gradient opt\n",
    "\n",
    "         ww, bb = fit_linreg_gradopt(X, yy, alpha)\n",
    "\n",
    "     Find weights and bias by using a gradient-based optimizer\n",
    "     (minimize_list) to improve the regularized least squares cost:\n",
    "\n",
    "       np.sum(((np.dot(X,ww) + bb) - yy)**2) + alpha*np.dot(ww,ww)\n",
    "\n",
    "     Inputs:\n",
    "             X N,D design matrix of input features\n",
    "            yy N,  real-valued targets\n",
    "         alpha     scalar regularization constant\n",
    "\n",
    "     Outputs:\n",
    "            ww D,  fitted weights\n",
    "            bb     scalar fitted bias\n",
    "    \"\"\"\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    init = (np.zeros(D), np.array(0))\n",
    "    ww, bb = minimize_list(linreg_cost, init, args)\n",
    "    return ww, bb\n",
    "\n",
    "\n",
    "def logreg_cost(params, X, yy, alpha):\n",
    "    \"\"\"Regularized logistic regression cost function and gradients\n",
    "\n",
    "    Can be optimized with minimize_list -- see fit_linreg_gradopt for a\n",
    "    demonstration of fitting a similar function.\n",
    "\n",
    "    Inputs:\n",
    "    params: tuple (ww, bb): weights ww (D,), bias bb scalar\n",
    "         X: N,D design matrix of input features\n",
    "        yy: N,  real-valued targets\n",
    "     alpha: regularization constant\n",
    "\n",
    "    Outputs: (E, [ww_bar, bb_bar]), cost and gradients\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb = params\n",
    "\n",
    "    # Force targets to be +/- 1\n",
    "    yy = 2*(yy==1) - 1\n",
    "\n",
    "    # forward computation of error\n",
    "    aa = yy*(np.dot(X, ww) + bb)\n",
    "    sigma = 1/(1 + np.exp(-aa))\n",
    "    E = -np.sum(np.log(sigma)) + alpha*np.dot(ww, ww)\n",
    "\n",
    "    # reverse computation of gradients\n",
    "    aa_bar = sigma - 1\n",
    "    bb_bar = np.dot(aa_bar, yy)\n",
    "    ww_bar = np.dot(X.T, yy*aa_bar) + 2*alpha*ww\n",
    "\n",
    "    return E, (ww_bar, bb_bar)\n",
    "\n",
    "\n",
    "def nn_cost(params, X, yy=None, alpha=None):\n",
    "    \"\"\"NN_COST simple neural network cost function and gradients, or predictions\n",
    "\n",
    "           E, params_bar = nn_cost([ww, bb, V, bk], X, yy, alpha)\n",
    "                    pred = nn_cost([ww, bb, V, bk], X)\n",
    "\n",
    "     Cost function E can be minimized with minimize_list\n",
    "\n",
    "     Inputs:\n",
    "             params (ww, bb, V, bk), where:\n",
    "                    --------------------------------\n",
    "                        ww K,  hidden-output weights\n",
    "                        bb     scalar output bias\n",
    "                        V  K,D hidden-input weights\n",
    "                        bk K,  hidden biases\n",
    "                    --------------------------------\n",
    "                  X N,D input design matrix\n",
    "                 yy N,  regression targets\n",
    "              alpha     scalar regularization for weights\n",
    "\n",
    "     Outputs:\n",
    "                     E  sum of squares error\n",
    "            params_bar  gradients wrt params, same format as params\n",
    "     OR\n",
    "               pred N,  predictions if only params and X are given as inputs\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb, V, bk = params\n",
    "\n",
    "    # Forwards computation of cost\n",
    "    A = np.dot(X, V.T) + bk[None,:] # N,K\n",
    "    P = 1 / (1 + np.exp(-A)) # N,K\n",
    "    F = np.dot(P, ww) + bb # N,\n",
    "    if yy is None:\n",
    "        # user wants prediction rather than training signal:\n",
    "        return F\n",
    "    res = F - yy # N,\n",
    "    E = np.dot(res, res) + alpha*(np.sum(V*V) + np.dot(ww,ww)) # 1x1\n",
    "\n",
    "    # Reverse computation of gradients\n",
    "    F_bar = 2*res # N,\n",
    "    ww_bar = np.dot(P.T, F_bar) + 2*alpha*ww # K,\n",
    "    bb_bar = np.sum(F_bar) # scalar\n",
    "    P_bar = np.dot(F_bar[:,None], ww[None,:]) # N,K\n",
    "    A_bar = P_bar * P * (1 - P) # N,K\n",
    "    V_bar = np.dot(A_bar.T, X) + 2*alpha*V # K,D\n",
    "    bk_bar = np.sum(A_bar, 0)\n",
    "\n",
    "    return E, (ww_bar, bb_bar, V_bar, bk_bar)\n",
    "\n",
    "\n",
    "def rbf_fn(X1, X2):\n",
    "    \"\"\"Helper routine for gp_post_par\"\"\"\n",
    "    return np.exp((np.dot(X1,(2*X2.T))-np.sum(X1*X1,1)[:,None]) - np.sum(X2*X2,1)[None,:])\n",
    "\n",
    "\n",
    "def gauss_kernel_fn(X1, X2, ell, sigma_f):\n",
    "    \"\"\"Helper routine for gp_post_par\"\"\"\n",
    "    return sigma_f**2 * rbf_fn(X1/(np.sqrt(2)*ell), X2/(np.sqrt(2)*ell))\n",
    "\n",
    "\n",
    "def gp_post_par(X_rest, X_obs, yy, sigma_y=0.05, ell=5.0, sigma_f=0.1):\n",
    "    \"\"\"GP_POST_PAR means and covariances of a posterior Gaussian process\n",
    "\n",
    "         rest_cond_mu, rest_cond_cov = gp_post_par(X_rest, X_obs, yy)\n",
    "         rest_cond_mu, rest_cond_cov = gp_post_par(X_rest, X_obs, yy, sigma_y, ell, sigma_f)\n",
    "\n",
    "     Calculate the means and covariances at all test locations of the posterior Gaussian\n",
    "     process conditioned on the observations yy at observed locations X_obs.\n",
    "\n",
    "     Inputs:\n",
    "                 X_rest GP test locations\n",
    "                  X_obs locations of observations\n",
    "                     yy observed values\n",
    "                sigma_y observation noise standard deviation\n",
    "                    ell kernel function length scale\n",
    "                sigma_f kernel function standard deviation\n",
    "\n",
    "     Outputs:\n",
    "           rest_cond_mu mean at each location in X_rest\n",
    "          rest_cond_cov covariance matrix between function values at all test locations\n",
    "    \"\"\"\n",
    "    X_rest = X_rest[:, None]\n",
    "    X_obs = X_obs[:, None]\n",
    "    K_rest = gauss_kernel_fn(X_rest, X_rest, ell, sigma_f)\n",
    "    K_rest_obs = gauss_kernel_fn(X_rest, X_obs, ell, sigma_f)\n",
    "    K_obs = gauss_kernel_fn(X_obs, X_obs, ell, sigma_f)\n",
    "    M = K_obs + sigma_y**2 * np.eye(yy.size)\n",
    "    M_cho, M_low = cho_factor(M)\n",
    "    rest_cond_mu = np.dot(K_rest_obs, cho_solve((M_cho, M_low), yy))\n",
    "    rest_cond_cov = K_rest - np.dot(K_rest_obs, cho_solve((M_cho, M_low), K_rest_obs.T))\n",
    "\n",
    "    return rest_cond_mu, rest_cond_cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./ct_data.npz')\n",
    "X_train = data['X_train']; X_val = data['X_val']; X_test = data['X_test']\n",
    "y_train = data['y_train']; y_val = data['y_val']; y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the training positions in y_train is -0.0000 and the standard error of the mean is 0.0050.\n",
      "The mean of the 5,785 positions in the y_val is -0.2160 and the standard error of the mean is  0.0129.\n",
      "The mean of the first 5,785 entries in the y_train is -0.4425 and the standard error of the mean is  0.0119.\n"
     ]
    }
   ],
   "source": [
    "print(f'The mean of the training positions in y_train is {np.mean(y_train):.4f} and the standard error of the mean is {np.std(y_train)/np.sqrt(len(y_train)):.4f}.')\n",
    "print(f'The mean of the 5,785 positions in the y_val is {np.mean(y_val):.4f} and the standard error of the mean is  {np.std(y_val)/np.sqrt(len(y_val)):.4f}.')\n",
    "print(f'The mean of the first 5,785 entries in the y_train is {np.mean(y_train[0:5785,]):.4f} and the standard error of the mean is  {np.std(y_train[0:5785,])/np.sqrt(5785):.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column indexes of constants: [ 59  69 179 189 351].\n",
      "Column indexes of dupulicates: [ 69  78  79 179 188 189 199 287 351 359].\n",
      "Column indexes for removal: [ 59  69  78  79 179 188 189 199 287 351 359].\n"
     ]
    }
   ],
   "source": [
    "# stage one: constant columns\n",
    "index1 = np.where(np.var(X_train, axis=0) == 0)[0]\n",
    "print(f'Column indexes of constants: {index1}.')\n",
    "\n",
    "# stage two: identical columns\n",
    "index2 = np.delete(np.arange(0, X_train.shape[1]), np.sort(np.unique(X_train, axis=1, return_index=True)[1]))\n",
    "print(f'Column indexes of dupulicates: {index2}.')\n",
    "\n",
    "# indeices for removal\n",
    "index_remove = np.unique(np.concatenate((index1, index2), axis=0))\n",
    "print(f'Column indexes for removal: {index_remove}.')\n",
    "\n",
    "# removal\n",
    "X_train = np.delete(X_train, index_remove, axis=1)\n",
    "X_val = np.delete(X_val, index_remove, axis=1)\n",
    "X_test = np.delete(X_test, index_remove, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linreg(X, yy, alpha):\n",
    "    n, d = X.shape[0], X.shape[1]\n",
    "    y_tilde = np.concatenate((yy, np.zeros(d)))\n",
    "    Phi = np.concatenate((X, np.ones((n, 1))), axis=1)\n",
    "    diag = np.diag(np.append(np.sqrt(alpha) * np.ones((d)), [0]))\n",
    "    Phi_tilde = np.concatenate((Phi, diag))\n",
    "    y_tilde = np.concatenate((yy, np.zeros((d+1))))\n",
    "    w_tilde = np.linalg.lstsq(Phi_tilde, y_tilde, rcond=None)[0]\n",
    "    ww, bb = w_tilde[0:-1], w_tilde[-1]\n",
    "    return ww, bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit X_train and y_train with alpha = 30 using least square method\n",
    "w_ls, b_ls = fit_linreg(X=X_train, yy=y_train, alpha=30)\n",
    "pred_ls = np.concatenate((X_train, np.ones((len(X_train), 1))), axis=1) @ np.concatenate((w_ls, b_ls), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit X_train and y_train with alpha = 30 using gradient descent method\n",
    "w_gd, b_gd = fit_linreg_gradopt(X=X_train, yy=y_train, alpha=30)\n",
    "pred_gd = np.concatenate((X_train, np.ones((len(X_train), 1))), axis=1) @ np.concatenate((w_ls, b_ls), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE using least square method is 0.3568.\n",
      "The RMSE using gradient descent method is 0.3568.\n"
     ]
    }
   ],
   "source": [
    "RMSE_ls = np.sqrt(np.mean((y_train - pred_ls) ** 2))\n",
    "RMSE_gd = np.sqrt(np.mean((y_train - pred_gd) ** 2))\n",
    "print(f'The RMSE using least square method is {RMSE_ls:.4f}.')\n",
    "print(f'The RMSE using gradient descent method is {RMSE_gd:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logreg_gradopt(X, yy, alpha):\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    init = (np.zeros(D), np.array([0]))\n",
    "    ww, bb = minimize_list(logreg_cost, init, args)\n",
    "    return ww, bb\n",
    "\n",
    "def logreg_prob(X, ww, bb):\n",
    "    return 1 / (1 + np.exp(- (X @ ww + bb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20 # number of thresholded classification problems to fit\n",
    "mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)\n",
    "prob_train = np.zeros((len(X_train), K))\n",
    "prob_val = np.zeros((len(X_val), K))\n",
    "w_logr = np.zeros((X_train.shape[1], K))\n",
    "b_logr = np.zeros(K)\n",
    "\n",
    "for kk in range(K):\n",
    "    labels = (y_train > thresholds[kk]).astype(int)\n",
    "    w_logr_kk, b_logr_kk = fit_logreg_gradopt(X=X_train, yy=labels, alpha=30)\n",
    "    w_logr[:,kk] = w_logr_kk\n",
    "    b_logr[kk] = b_logr_kk[0]\n",
    "    prob_train[:,kk] = logreg_prob(X=X_train, ww=w_logr[:,kk], bb=b_logr[kk])\n",
    "    prob_val[:,kk] = logreg_prob(X=X_val, ww=w_logr[:,kk], bb=b_logr[kk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_logr_lr, b_logr_lr = fit_linreg_gradopt(X=prob_train, yy=y_train, alpha=30)\n",
    "param_logr_lr = np.concatenate((w_logr_lr, b_logr_lr), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE of the TRAIN set is 0.1544.\n"
     ]
    }
   ],
   "source": [
    "pred_logr_lr_train = np.concatenate((prob_train, np.ones((len(prob_train), 1))), axis=1) @ param_logr_lr\n",
    "RMSE_log_lr_train = np.sqrt(np.mean((y_train - pred_logr_lr_train) ** 2))\n",
    "print(f'The RMSE of the TRAIN set is {RMSE_log_lr_train:.4f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE of the VAL set is 0.2542.\n"
     ]
    }
   ],
   "source": [
    "pred_log_lr_val = np.concatenate((prob_val, np.ones((len(prob_val), 1))), axis=1) @ param_logr_lr\n",
    "RMSE_log_lr_val = np.sqrt(np.mean((y_val - pred_log_lr_val) ** 2))\n",
    "print(f'The RMSE of the VAL set is {RMSE_log_lr_val:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE of nn with random initialisaiton using the TRAIN set is 0.1411.\n"
     ]
    }
   ],
   "source": [
    "# random initialisation\n",
    "d, k = X_train.shape[1], 20\n",
    "ww = 0.1*np.random.randn(k)/np.sqrt(k)\n",
    "bb = 0.1*np.random.randn()/np.sqrt(k)\n",
    "V = 0.1*np.random.randn(k, d)/np.sqrt(k)\n",
    "bk = 0.1*np.random.randn(k)/np.sqrt(k)\n",
    "nn_init_random = (ww, bb, V, bk)\n",
    "\n",
    "# X_train, y_train and alpha\n",
    "nn_alpha = 30\n",
    "nn_args = (X_train, y_train, nn_alpha)\n",
    "\n",
    "nn_param_random = minimize_list(nn_cost, nn_init_random, nn_args)\n",
    "\n",
    "pred_nn_random_train = nn_cost(nn_param_random, X=X_train)\n",
    "RMSE_nn_random_train = np.sqrt(np.mean((y_train - pred_nn_random_train) ** 2))\n",
    "\n",
    "print(f'The RMSE of nn with random initialisaiton using the TRAIN set is {RMSE_nn_random_train:.4f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE of nn with Q3 initialisaiton using the TRAIN set is 0.1394.\n"
     ]
    }
   ],
   "source": [
    "# Q3 initialisation\n",
    "nn_init_q3 = (w_logr_lr, b_logr_lr, w_logr.T, b_logr)\n",
    "\n",
    "# X_train, y_train and alpha\n",
    "nn_alpha = 30\n",
    "nn_args = (X_train, y_train, nn_alpha)\n",
    "\n",
    "nn_params_q3 = minimize_list(nn_cost, nn_init_q3, nn_args)\n",
    "\n",
    "pred_nn_q3_train = nn_cost(nn_params_q3, X=X_train)\n",
    "RMSE_nn_q3_train = np.sqrt(np.mean((y_train - pred_nn_q3_train) ** 2))\n",
    "\n",
    "print(f'The RMSE of nn with Q3 initialisaiton using the TRAIN set is {RMSE_nn_q3_train:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_reg(XX, XX_val, yy, yy_val, params, alpha, report_train=False, report_val=True):\n",
    "    args = (XX, yy, alpha)\n",
    "    params_bar = minimize_list(nn_cost, params, args)\n",
    "    pred_train = nn_cost(params_bar, X=XX)\n",
    "    RMSE_train = np.sqrt(np.mean((yy - pred_train) ** 2))\n",
    "    pred_val = nn_cost(params_bar, X=XX_val)\n",
    "    RMSE_val = np.sqrt(np.mean((yy_val - pred_val) ** 2))\n",
    "    if report_train:\n",
    "        print(f'The RMSE_train is {RMSE_train:.4f} while alpha is {alpha:.2f}.')\n",
    "    if report_val:\n",
    "        print(f'The RMSE_val is {RMSE_val:.4f} while alpha is {alpha:.2f}.')\n",
    "    return RMSE_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE_val is 0.2703 while alpha is 30.00.\n",
      "The above RMSE is a baseline. \n",
      "\n",
      "---------- Iteration 1 ----------\n",
      "The RMSE_val is 0.2734 while alpha is 37.24.\n",
      "The RMSE_val is 0.2486 while alpha is 3.80.\n",
      "The RMSE_val is 0.2733 while alpha is 37.84.\n",
      "The maximal PI is 0.3537 while alpha equals to 3.78.\n",
      "The RMSE_val is 0.2573 while alpha is 3.78.\n",
      "\n",
      "\n",
      "---------- Iteration 2 ----------\n",
      "The RMSE_val is 0.2800 while alpha is 46.70.\n",
      "The RMSE_val is 0.2561 while alpha is 13.08.\n",
      "The RMSE_val is 0.2719 while alpha is 32.56.\n",
      "The maximal PI is 0.3414 while alpha equals to 7.62.\n",
      "The RMSE_val is 0.2451 while alpha is 7.62.\n",
      "\n",
      "\n",
      "---------- Iteration 3 ----------\n",
      "The RMSE_val is 0.2709 while alpha is 31.04.\n",
      "The RMSE_val is 0.2436 while alpha is 0.32.\n",
      "The RMSE_val is 0.2663 while alpha is 18.10.\n",
      "The maximal PI is 0.2588 while alpha equals to 0.00.\n",
      "The RMSE_val is 0.2576 while alpha is 0.00.\n",
      "\n",
      "\n",
      "---------- Iteration 4 ----------\n",
      "The RMSE_val is 0.2598 while alpha is 15.58.\n",
      "The RMSE_val is 0.2718 while alpha is 31.70.\n",
      "The RMSE_val is 0.2717 while alpha is 32.04.\n",
      "The maximal PI is 0.2389 while alpha equals to 8.42.\n",
      "The RMSE_val is 0.2512 while alpha is 8.42.\n",
      "\n",
      "\n",
      "---------- Iteration 5 ----------\n",
      "The RMSE_val is 0.2789 while alpha is 45.66.\n",
      "The RMSE_val is 0.2653 while alpha is 21.60.\n",
      "The RMSE_val is 0.2552 while alpha is 0.96.\n",
      "The maximal PI is 0.1766 while alpha equals to 8.36.\n",
      "The RMSE_val is 0.2510 while alpha is 8.36.\n",
      "\n",
      "\n",
      "The best RMSE of the VAL set is 0.2436 while alpha is 18.10.\n"
     ]
    }
   ],
   "source": [
    "# alpha set\n",
    "alpha_set = np.arange(0, 50, 0.02)\n",
    "# Q3 initialisation\n",
    "nn_init_q3 = (w_logr_lr, b_logr_lr, w_logr.T, b_logr)\n",
    "points_num = 3\n",
    "alpha_init = 30\n",
    "RMSE_alpha_base = train_nn_reg(XX=X_train, XX_val=X_val, yy=y_train, yy_val=y_val, params=nn_init_q3, alpha=alpha_init)\n",
    "print('The above RMSE is a baseline. \\n')\n",
    "best_RMSE_val = RMSE_alpha_base\n",
    "best_alpha = alpha_init\n",
    "\n",
    "for i in range(5):\n",
    "    pick = np.random.randint(0, len(alpha_set), points_num)\n",
    "    alpha_train = alpha_set[pick]\n",
    "    alpha_acquisition = np.delete(alpha_set, pick)\n",
    "    alpha_set = np.delete(alpha_set, pick)\n",
    "\n",
    "    print(f'---------- Iteration {i+1} ----------')\n",
    "    for j in range(points_num):\n",
    "        RMSE_found = np.append(RMSE_found, train_nn_reg(XX=X_train, XX_val=X_val, yy=y_train, yy_val=y_val, params=nn_init_q3, alpha=alpha_train[j]))\n",
    "        alpha_found = np.append(alpha_found, alpha_train[j])\n",
    "        \n",
    "    y = np.log(RMSE_alpha_base) - np.log(RMSE_found)\n",
    "    # mu, cov = gp_post_par(X_rest=alpha_acquisition, X_obs=alpha_train, yy=y)\n",
    "    mu, cov = gp_post_par(X_rest=alpha_acquisition, X_obs=alpha_found, yy=y)\n",
    "    pi = scipy.stats.norm.cdf((mu - y.max()) / np.sqrt(np.diag(cov)))\n",
    "    alpha_opt = alpha_acquisition[np.argmax(pi)]\n",
    "    print(f'The maximal PI is {pi.max():.4f} while alpha equals to {alpha_opt:.2f}.')\n",
    "\n",
    "    alpha_set = np.delete(alpha_set, np.where(alpha_set == alpha_opt))\n",
    "    RMSE_found = np.append(RMSE_found, train_nn_reg(XX=X_train, XX_val=X_val, yy=y_train, yy_val=y_val, params=nn_init_q3, alpha=alpha_opt))\n",
    "    alpha_found = np.append(alpha_found, alpha_opt)\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "best_RMSE_val = RMSE_found.min()\n",
    "best_alpha = alpha_found[np.where(RMSE_found == RMSE_found.min())][0]\n",
    "print(f'The best RMSE of the VAL set is {best_RMSE_val:.4f} while alpha is {best_alpha:.2f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # alpha set\n",
    "# alpha_set = np.arange(0, 50, 0.02)\n",
    "# # Q3 initialisation\n",
    "# nn_init_q3 = (w_logr_lr, b_logr_lr, w_logr.T, b_logr)\n",
    "# points_num = 3\n",
    "# RMSE_alpha_base = train_nn_reg(XX=X_train, XX_val=X_val, yy=y_train, yy_val=y_val, params=nn_init_q3, alpha=30)\n",
    "# print('The above RMSE is a baseline.')\n",
    "\n",
    "# for i in range(5):\n",
    "#     pick = np.random.randint(0, len(alpha_set), points_num)\n",
    "#     alpha_train = alpha_set[pick]\n",
    "#     alpha_acquisition = np.delete(alpha_set, pick)\n",
    "#     RMSE_val = np.zeros(points_num)\n",
    "#     for j in range(points_num):\n",
    "#         RMSE_val[j] = train_nn_reg(XX=X_train, XX_val=X_val, yy=y_train, yy_val=y_val, params=nn_init_q3, alpha=alpha_train[j])\n",
    "#     y = np.log(RMSE_alpha_base) - np.log(RMSE_val)\n",
    "    \n",
    "#     mu, cov = gp_post_par(X_rest=alpha_acquisition, X_obs=alpha_train, yy=y)\n",
    "#     pi = scipy.stats.norm.cdf((mu - y.max()) / np.sqrt(np.diag(cov)))\n",
    "#     alpha_opt = alpha_acquisition[np.argmax(pi)]\n",
    "#     print(f'Iteration {i+1}, the maximal PI is {pi.max()} while alpha equals to {alpha_opt:.2f}.')\n",
    "#     RMSE_alpha_base = train_nn_reg(XX=X_train, XX_val=X_val, yy=y_train, yy_val=y_val, params=nn_init_q3, alpha=alpha_opt)\n",
    "#     print('The above RMSE is a new baseline.')\n",
    "#     # alpha_set = np.append(alpha_set, alpha_acquisition[np.argmax(PI)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE_train is 0.0658 while alpha is 0.32.\n",
      "The RMSE_val is 0.2436 while alpha is 0.32.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24361278595360691"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nn_reg(XX=X_train, XX_val=X_val, yy=y_train, yy_val=y_val, params=nn_init_q3, \n",
    "             alpha=best_alpha, report_train=True, report_val=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
