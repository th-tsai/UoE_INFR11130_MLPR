{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import cho_factor, cho_solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_unwrap(param_vec, shapes, sizes):\n",
    "    \"\"\"Helper routine for minimize_list\"\"\"\n",
    "    args = []\n",
    "    pos = 0\n",
    "    for i in range(len(shapes)):\n",
    "        sz = sizes[i]\n",
    "        args.append(param_vec[pos:pos+sz].reshape(shapes[i]))\n",
    "        pos += sz\n",
    "    return args\n",
    "\n",
    "\n",
    "def params_wrap(param_list):\n",
    "    \"\"\"Helper routine for minimize_list\"\"\"\n",
    "    param_list = [np.array(x) for x in param_list]\n",
    "    shapes = [x.shape for x in param_list]\n",
    "    sizes = [x.size for x in param_list]\n",
    "    param_vec = np.zeros(sum(sizes))\n",
    "    pos = 0\n",
    "    for param in param_list:\n",
    "        sz = param.size\n",
    "        param_vec[pos:pos+sz] = param.ravel()\n",
    "        pos += sz\n",
    "    unwrap = lambda pvec: params_unwrap(pvec, shapes, sizes)\n",
    "    return param_vec, unwrap\n",
    "\n",
    "\n",
    "def minimize_list(cost, init_list, args):\n",
    "    \"\"\"Optimize a list of arrays (wrapper of scipy.optimize.minimize)\n",
    "\n",
    "    The input function \"cost\" should take a list of parameters,\n",
    "    followed by any extra arguments:\n",
    "        cost(init_list, *args)\n",
    "    should return the cost of the initial condition, and a list in the same\n",
    "    format as init_list giving gradients of the cost wrt the parameters.\n",
    "\n",
    "    The options to the optimizer have been hard-coded. You may wish\n",
    "    to change disp to True to get more diagnostics. You may want to\n",
    "    decrease maxiter while debugging. Although please report all results\n",
    "    in Q2-5 using maxiter=500.\n",
    "    \"\"\"\n",
    "    opt = {'maxiter': 500, 'disp': False}\n",
    "    init, unwrap = params_wrap(init_list)\n",
    "    def wrap_cost(vec, *args):\n",
    "        E, params_bar = cost(unwrap(vec), *args)\n",
    "        vec_bar, _ = params_wrap(params_bar)\n",
    "        return E, vec_bar\n",
    "    res = minimize(wrap_cost, init, args, 'L-BFGS-B', jac=True, options=opt)\n",
    "    return unwrap(res.x)\n",
    "\n",
    "\n",
    "def linreg_cost(params, X, yy, alpha):\n",
    "    \"\"\"Regularized least squares cost function and gradients\n",
    "\n",
    "    Can be optimized with minimize_list -- see fit_linreg_gradopt for a\n",
    "    demonstration.\n",
    "\n",
    "    Inputs:\n",
    "    params: tuple (ww, bb): weights ww (D,), bias bb scalar\n",
    "         X: N,D design matrix of input features\n",
    "        yy: N,  real-valued targets\n",
    "     alpha: regularization constant\n",
    "\n",
    "    Outputs: (E, [ww_bar, bb_bar]), cost and gradients\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb = params\n",
    "\n",
    "    # forward computation of error\n",
    "    ff = np.dot(X, ww) + bb\n",
    "    res = ff - yy\n",
    "    E = np.dot(res, res) + alpha*np.dot(ww, ww)\n",
    "\n",
    "    # reverse computation of gradients\n",
    "    ff_bar = 2*res\n",
    "    bb_bar = np.sum(ff_bar)\n",
    "    ww_bar = np.dot(X.T, ff_bar) + 2*alpha*ww\n",
    "\n",
    "    return E, [ww_bar, bb_bar]\n",
    "\n",
    "\n",
    "def fit_linreg_gradopt(X, yy, alpha):\n",
    "    \"\"\"\n",
    "    fit a regularized linear regression model with gradient opt\n",
    "\n",
    "         ww, bb = fit_linreg_gradopt(X, yy, alpha)\n",
    "\n",
    "     Find weights and bias by using a gradient-based optimizer\n",
    "     (minimize_list) to improve the regularized least squares cost:\n",
    "\n",
    "       np.sum(((np.dot(X,ww) + bb) - yy)**2) + alpha*np.dot(ww,ww)\n",
    "\n",
    "     Inputs:\n",
    "             X N,D design matrix of input features\n",
    "            yy N,  real-valued targets\n",
    "         alpha     scalar regularization constant\n",
    "\n",
    "     Outputs:\n",
    "            ww D,  fitted weights\n",
    "            bb     scalar fitted bias\n",
    "    \"\"\"\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    init = (np.zeros(D), np.array(0))\n",
    "    ww, bb = minimize_list(linreg_cost, init, args)\n",
    "    return ww, bb\n",
    "\n",
    "\n",
    "def logreg_cost(params, X, yy, alpha):\n",
    "    \"\"\"Regularized logistic regression cost function and gradients\n",
    "\n",
    "    Can be optimized with minimize_list -- see fit_linreg_gradopt for a\n",
    "    demonstration of fitting a similar function.\n",
    "\n",
    "    Inputs:\n",
    "    params: tuple (ww, bb): weights ww (D,), bias bb scalar\n",
    "         X: N,D design matrix of input features\n",
    "        yy: N,  real-valued targets\n",
    "     alpha: regularization constant\n",
    "\n",
    "    Outputs: (E, [ww_bar, bb_bar]), cost and gradients\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb = params\n",
    "\n",
    "    # Force targets to be +/- 1\n",
    "    yy = 2*(yy==1) - 1\n",
    "\n",
    "    # forward computation of error\n",
    "    aa = yy*(np.dot(X, ww) + bb)\n",
    "    sigma = 1/(1 + np.exp(-aa))\n",
    "    E = -np.sum(np.log(sigma)) + alpha*np.dot(ww, ww)\n",
    "\n",
    "    # reverse computation of gradients\n",
    "    aa_bar = sigma - 1\n",
    "    bb_bar = np.dot(aa_bar, yy)\n",
    "    ww_bar = np.dot(X.T, yy*aa_bar) + 2*alpha*ww\n",
    "\n",
    "    return E, (ww_bar, bb_bar)\n",
    "\n",
    "\n",
    "def nn_cost(params, X, yy=None, alpha=None):\n",
    "    \"\"\"NN_COST simple neural network cost function and gradients, or predictions\n",
    "\n",
    "           E, params_bar = nn_cost([ww, bb, V, bk], X, yy, alpha)\n",
    "                    pred = nn_cost([ww, bb, V, bk], X)\n",
    "\n",
    "     Cost function E can be minimized with minimize_list\n",
    "\n",
    "     Inputs:\n",
    "             params (ww, bb, V, bk), where:\n",
    "                    --------------------------------\n",
    "                        ww K,  hidden-output weights\n",
    "                        bb     scalar output bias\n",
    "                        V  K,D hidden-input weights\n",
    "                        bk K,  hidden biases\n",
    "                    --------------------------------\n",
    "                  X N,D input design matrix\n",
    "                 yy N,  regression targets\n",
    "              alpha     scalar regularization for weights\n",
    "\n",
    "     Outputs:\n",
    "                     E  sum of squares error\n",
    "            params_bar  gradients wrt params, same format as params\n",
    "     OR\n",
    "               pred N,  predictions if only params and X are given as inputs\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb, V, bk = params\n",
    "\n",
    "    # Forwards computation of cost\n",
    "    A = np.dot(X, V.T) + bk[None,:] # N,K\n",
    "    P = 1 / (1 + np.exp(-A)) # N,K\n",
    "    F = np.dot(P, ww) + bb # N,\n",
    "    if yy is None:\n",
    "        # user wants prediction rather than training signal:\n",
    "        return F\n",
    "    res = F - yy # N,\n",
    "    E = np.dot(res, res) + alpha*(np.sum(V*V) + np.dot(ww,ww)) # 1x1\n",
    "\n",
    "    # Reverse computation of gradients\n",
    "    F_bar = 2*res # N,\n",
    "    ww_bar = np.dot(P.T, F_bar) + 2*alpha*ww # K,\n",
    "    bb_bar = np.sum(F_bar) # scalar\n",
    "    P_bar = np.dot(F_bar[:,None], ww[None,:]) # N,K\n",
    "    A_bar = P_bar * P * (1 - P) # N,K\n",
    "    V_bar = np.dot(A_bar.T, X) + 2*alpha*V # K,D\n",
    "    bk_bar = np.sum(A_bar, 0)\n",
    "\n",
    "    return E, (ww_bar, bb_bar, V_bar, bk_bar)\n",
    "\n",
    "\n",
    "def rbf_fn(X1, X2):\n",
    "    \"\"\"Helper routine for gp_post_par\"\"\"\n",
    "    return np.exp((np.dot(X1,(2*X2.T))-np.sum(X1*X1,1)[:,None]) - np.sum(X2*X2,1)[None,:])\n",
    "\n",
    "\n",
    "def gauss_kernel_fn(X1, X2, ell, sigma_f):\n",
    "    \"\"\"Helper routine for gp_post_par\"\"\"\n",
    "    return sigma_f**2 * rbf_fn(X1/(np.sqrt(2)*ell), X2/(np.sqrt(2)*ell))\n",
    "\n",
    "\n",
    "def gp_post_par(X_rest, X_obs, yy, sigma_y=0.05, ell=5.0, sigma_f=0.1):\n",
    "    \"\"\"GP_POST_PAR means and covariances of a posterior Gaussian process\n",
    "\n",
    "         rest_cond_mu, rest_cond_cov = gp_post_par(X_rest, X_obs, yy)\n",
    "         rest_cond_mu, rest_cond_cov = gp_post_par(X_rest, X_obs, yy, sigma_y, ell, sigma_f)\n",
    "\n",
    "     Calculate the means and covariances at all test locations of the posterior Gaussian\n",
    "     process conditioned on the observations yy at observed locations X_obs.\n",
    "\n",
    "     Inputs:\n",
    "                 X_rest GP test locations\n",
    "                  X_obs locations of observations\n",
    "                     yy observed values\n",
    "                sigma_y observation noise standard deviation\n",
    "                    ell kernel function length scale\n",
    "                sigma_f kernel function standard deviation\n",
    "\n",
    "     Outputs:\n",
    "           rest_cond_mu mean at each location in X_rest\n",
    "          rest_cond_cov covariance matrix between function values at all test locations\n",
    "    \"\"\"\n",
    "    X_rest = X_rest[:, None]\n",
    "    X_obs = X_obs[:, None]\n",
    "    K_rest = gauss_kernel_fn(X_rest, X_rest, ell, sigma_f)\n",
    "    K_rest_obs = gauss_kernel_fn(X_rest, X_obs, ell, sigma_f)\n",
    "    K_obs = gauss_kernel_fn(X_obs, X_obs, ell, sigma_f)\n",
    "    M = K_obs + sigma_y**2 * np.eye(yy.size)\n",
    "    M_cho, M_low = cho_factor(M)\n",
    "    rest_cond_mu = np.dot(K_rest_obs, cho_solve((M_cho, M_low), yy))\n",
    "    rest_cond_cov = K_rest - np.dot(K_rest_obs, cho_solve((M_cho, M_low), K_rest_obs.T))\n",
    "\n",
    "    return rest_cond_mu, rest_cond_cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./ct_data.npz')\n",
    "X_train = data['X_train']; X_val = data['X_val']; X_test = data['X_test']\n",
    "y_train = data['y_train']; y_val = data['y_val']; y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage one: constant columns\n",
    "index1 = np.where(np.var(X_train, axis=0) == 0)[0]\n",
    "# print(f'Column indexes of constants: {index1}.')\n",
    "\n",
    "# stage two: identical columns\n",
    "index2 = np.delete(np.arange(0, X_train.shape[1]), np.sort(np.unique(X_train, axis=1, return_index=True)[1]))\n",
    "# print(f'Column indexes of dupulicates: {index2}.')\n",
    "\n",
    "# indeices for removal\n",
    "index_remove = np.unique(np.concatenate((index1, index2), axis=0))\n",
    "# print(f'Column indexes for removal: {index_remove}.')\n",
    "\n",
    "# removal\n",
    "X_train = np.delete(X_train, index_remove, axis=1)\n",
    "X_val = np.delete(X_val, index_remove, axis=1)\n",
    "X_test = np.delete(X_test, index_remove, axis=1)\n",
    "\n",
    "X_train = np.delete(X_train, 271, axis=1)\n",
    "X_val = np.delete(X_val, 271, axis=1)\n",
    "X_test = np.delete(X_test, 27, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this test, the data would delete column 271 due to some investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linreg(X, yy, alpha):\n",
    "    n, d = X.shape[0], X.shape[1]\n",
    "    y_tilde = np.concatenate((yy, np.zeros(d)))\n",
    "    Phi = np.concatenate((X, np.ones((n, 1))), axis=1)\n",
    "    diag = np.diag(np.append(np.sqrt(alpha) * np.ones((d)), [0]))\n",
    "    Phi_tilde = np.concatenate((Phi, diag))\n",
    "    y_tilde = np.concatenate((yy, np.zeros((d+1))))\n",
    "    w_tilde = np.linalg.lstsq(Phi_tilde, y_tilde, rcond=None)[0]\n",
    "    ww, bb = w_tilde[0:-1], w_tilde[-1]\n",
    "    return ww, bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logreg_gradopt(X, yy, alpha):\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    init = (np.zeros(D), np.array(0))\n",
    "    ww, bb = minimize_list(logreg_cost, init, args)\n",
    "    return ww, bb\n",
    "\n",
    "def logreg_prob(X, ww, bb):\n",
    "    return 1 / (1 + np.exp(- (X @ ww + bb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_reg(XX, XX_val, yy, yy_val, params, alpha, report_train=False, report_val=True):\n",
    "    args = (XX_val, yy_val, alpha)\n",
    "    params_bar = minimize_list(nn_cost, params, args)\n",
    "    pred = nn_cost(params_bar, X=XX)\n",
    "    RMSE = np.sqrt(np.mean((yy - pred) ** 2))\n",
    "    pred_val = nn_cost(params_bar, X=XX_val)\n",
    "    RMSE_val = np.sqrt(np.mean((yy_val - pred_val) ** 2))\n",
    "    if report_train:\n",
    "        print(f'The RMSE_train is {RMSE:.4f} while alpha is {alpha}.')\n",
    "    if report_val:\n",
    "        print(f'The RMSE_val is {RMSE_val:.4f} while alpha is {alpha}.')\n",
    "    return RMSE_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first use standardised data to repeat the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardisation\n",
    "X_train_std = (X_train - X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "X_val_std = (X_val - X_train.mean(axis=0)) / X_train.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE of TRAIN set is 0.0903.\n",
      "The RMSE of VAL set is 0.2445.\n"
     ]
    }
   ],
   "source": [
    "K = 20 # number of thresholded classification problems to fit\n",
    "mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)\n",
    "prob_train_std = np.zeros((len(X_train_std), K))\n",
    "prob_val_std = np.zeros((len(X_val_std), K))\n",
    "logr_w_std = np.zeros((X_train_std.shape[1], K))\n",
    "logr_b_std = np.zeros(K)\n",
    "\n",
    "for kk in range(K):\n",
    "    labels = (y_train > thresholds[kk]).astype(int)\n",
    "    w_logr_std, b_logr_std = fit_logreg_gradopt(X=X_train_std, yy=labels, alpha=30)\n",
    "    logr_w_std[:,kk] = w_logr_std\n",
    "    logr_b_std[kk] = b_logr_std\n",
    "    prob_train_std[:,kk] = logreg_prob(X=X_train_std, ww=w_logr_std, bb=b_logr_std)\n",
    "    prob_val_std[:,kk] = logreg_prob(X=X_val_std, ww=w_logr_std, bb=b_logr_std)\n",
    "\n",
    "lr_w_std, lr_b_std = fit_linreg(X=prob_train_std, yy=y_train, alpha=30)\n",
    "lr_para_std = np.concatenate((lr_w_std, lr_b_std), axis=None)\n",
    "pred_train_std = np.concatenate((prob_train_std, np.ones((len(prob_train_std), 1))), axis=1) @ lr_para_std\n",
    "RMSE_train_std = np.sqrt(np.mean((y_train - pred_train_std) ** 2))\n",
    "pred_val_std = np.concatenate((prob_val_std, np.ones((len(prob_val_std), 1))), axis=1) @ lr_para_std\n",
    "RMSE_val_std = np.sqrt(np.mean((y_val - pred_val_std) ** 2))\n",
    "\n",
    "print(f'The RMSE of TRAIN set is {RMSE_train_std:.4f}.')\n",
    "print(f'The RMSE of VAL set is {RMSE_val_std:.4f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE of nn using TRAIN set is 0.0874.\n"
     ]
    }
   ],
   "source": [
    "# params form above\n",
    "nn_init_std = (lr_w_std, lr_b_std, logr_w_std.T, logr_b_std)\n",
    "\n",
    "# X_train_std, y_train and alpha\n",
    "nn_alpha = 30\n",
    "nn_args_std = (X_train_std, y_train, nn_alpha)\n",
    "\n",
    "# nn params\n",
    "nn_params_std = minimize_list(nn_cost, nn_init_std, nn_args_std)\n",
    "\n",
    "pred_nn_train_std = nn_cost(nn_params_std, X=X_train_std)\n",
    "RMSE_nn_train_std = np.sqrt(np.mean((y_train - pred_nn_train_std) ** 2))\n",
    "\n",
    "print(f'The RMSE of nn using TRAIN set is {RMSE_nn_train_std:.4f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE_val is 0.0923 while alpha is 30.\n",
      "The RMSE_val is 0.0777 while alpha is 20.400000000000002.\n",
      "The RMSE_val is 0.0766 while alpha is 19.02.\n",
      "The RMSE_val is 0.0876 while alpha is 24.84.\n",
      "Iteration 1, the maximal PI is 0.24948331698853637 with alpha equals to 18.0.\n",
      "The RMSE_val is 0.0753 while alpha is 18.0.\n",
      "The RMSE_val is 0.0355 while alpha is 3.08.\n",
      "The RMSE_val is 0.0765 while alpha is 18.86.\n",
      "The RMSE_val is 0.1004 while alpha is 38.480000000000004.\n",
      "Iteration 2, the maximal PI is 0.00038244600012522344 with alpha equals to 2.7800000000000002.\n",
      "The RMSE_val is 0.0359 while alpha is 2.7800000000000002.\n",
      "The RMSE_val is 0.0314 while alpha is 2.2.\n",
      "The RMSE_val is 0.0889 while alpha is 26.18.\n",
      "The RMSE_val is 0.0387 while alpha is 3.9.\n",
      "Iteration 3, the maximal PI is 0.1739506542324482 with alpha equals to 0.0.\n",
      "The RMSE_val is 0.0204 while alpha is 0.0.\n",
      "The RMSE_val is 0.0693 while alpha is 15.18.\n",
      "The RMSE_val is 0.0751 while alpha is 18.56.\n",
      "The RMSE_val is 0.1035 while alpha is 41.46.\n",
      "Iteration 4, the maximal PI is 1.0 with alpha equals to 0.0.\n",
      "The RMSE_val is 0.0204 while alpha is 0.0.\n",
      "The RMSE_val is 0.0896 while alpha is 27.16.\n",
      "The RMSE_val is 0.0713 while alpha is 16.14.\n",
      "The RMSE_val is 0.0642 while alpha is 12.82.\n",
      "Iteration 5, the maximal PI is 1.0 with alpha equals to 0.0.\n",
      "The RMSE_val is 0.0204 while alpha is 0.0.\n"
     ]
    }
   ],
   "source": [
    "# initialation\n",
    "alpha_set = np.arange(0, 50, 0.02)\n",
    "points_num = 3\n",
    "RMSE_alpha_base_std = train_nn_reg(XX=X_train_std, XX_val=X_val_std, yy=y_train, yy_val=y_val, params=nn_init_std, alpha=30)\n",
    "\n",
    "for i in range(5):\n",
    "    pick = np.random.randint(0, len(alpha_set), points_num)\n",
    "    alpha_train = alpha_set[pick]\n",
    "    alpha_acquisition = np.delete(alpha_set, pick)\n",
    "    RMSE_val = np.zeros(points_num)\n",
    "    for j in range(points_num):\n",
    "        RMSE_val[j] = train_nn_reg(XX=X_train_std, XX_val=X_val_std, yy=y_train, yy_val=y_val, params=nn_init_std, alpha=alpha_train[j])\n",
    "    y = np.log(RMSE_alpha_base_std) - np.log(RMSE_val)\n",
    "    mu, cov = gp_post_par(X_rest=alpha_acquisition, X_obs=alpha_train, yy=y)\n",
    "    sigma = np.sqrt(np.diag(cov))\n",
    "    PI = scipy.stats.norm.cdf((mu - y.max()) / sigma)\n",
    "    print(f'Iteration {i+1}, the maximal PI is {PI.max()} with alpha equals to {alpha_acquisition[np.argmax(PI)]}.')\n",
    "    RMSE_alpha_base_std = train_nn_reg(XX=X_train_std, XX_val=X_val_std, yy=y_train, yy_val=y_val, params=nn_init_std, alpha=alpha_acquisition[np.argmax(PI)])\n",
    "    alpha_set = np.append(alpha_set, alpha_acquisition[np.argmax(PI)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE_train is 0.2111 while alpha is 0.0.\n",
      "The RMSE_val is 0.0204 while alpha is 0.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.020414185082332455"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_alpha_opt = alpha_acquisition[np.argmax(PI)]\n",
    "train_nn_reg(XX=X_train_std, XX_val=X_val_std, yy=y_train, yy_val=y_val, \n",
    "             params=nn_init_std, alpha=nn_alpha_opt, report_train=True, report_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE of nn using TRAIN set is 0.0503.\n",
      "The RMSE of nn using VAL set is 0.2611.\n"
     ]
    }
   ],
   "source": [
    "nn_args_std_opta = (X_train_std, y_train, nn_alpha_opt)\n",
    "\n",
    "nn_params_std_opta = minimize_list(nn_cost, nn_init_std, nn_args_std_opta)\n",
    "\n",
    "pred_nn_train_std_opta = nn_cost(nn_params_std_opta, X=X_train_std)\n",
    "RMSE_nn_train_std_opta = np.sqrt(np.mean((y_train - pred_nn_train_std_opta) ** 2))\n",
    "\n",
    "pred_nn_val_std_opta = nn_cost(nn_params_std_opta, X=X_val_std)\n",
    "RMSE_nn_val_std_opta = np.sqrt(np.mean((y_val - pred_nn_val_std_opta) ** 2))\n",
    "\n",
    "print(f'The RMSE of nn using TRAIN set is {RMSE_nn_train_std_opta:.4f}.')\n",
    "print(f'The RMSE of nn using VAL set is {RMSE_nn_val_std_opta:.4f}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
