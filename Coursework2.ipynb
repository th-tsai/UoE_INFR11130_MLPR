{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import cho_factor, cho_solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_unwrap(param_vec, shapes, sizes):\n",
    "    \"\"\"Helper routine for minimize_list\"\"\"\n",
    "    args = []\n",
    "    pos = 0\n",
    "    for i in range(len(shapes)):\n",
    "        sz = sizes[i]\n",
    "        args.append(param_vec[pos:pos+sz].reshape(shapes[i]))\n",
    "        pos += sz\n",
    "    return args\n",
    "\n",
    "\n",
    "def params_wrap(param_list):\n",
    "    \"\"\"Helper routine for minimize_list\"\"\"\n",
    "    param_list = [np.array(x) for x in param_list]\n",
    "    shapes = [x.shape for x in param_list]\n",
    "    sizes = [x.size for x in param_list]\n",
    "    param_vec = np.zeros(sum(sizes))\n",
    "    pos = 0\n",
    "    for param in param_list:\n",
    "        sz = param.size\n",
    "        param_vec[pos:pos+sz] = param.ravel()\n",
    "        pos += sz\n",
    "    unwrap = lambda pvec: params_unwrap(pvec, shapes, sizes)\n",
    "    return param_vec, unwrap\n",
    "\n",
    "\n",
    "def minimize_list(cost, init_list, args):\n",
    "    \"\"\"Optimize a list of arrays (wrapper of scipy.optimize.minimize)\n",
    "\n",
    "    The input function \"cost\" should take a list of parameters,\n",
    "    followed by any extra arguments:\n",
    "        cost(init_list, *args)\n",
    "    should return the cost of the initial condition, and a list in the same\n",
    "    format as init_list giving gradients of the cost wrt the parameters.\n",
    "\n",
    "    The options to the optimizer have been hard-coded. You may wish\n",
    "    to change disp to True to get more diagnostics. You may want to\n",
    "    decrease maxiter while debugging. Although please report all results\n",
    "    in Q2-5 using maxiter=500.\n",
    "    \"\"\"\n",
    "    opt = {'maxiter': 500, 'disp': False}\n",
    "    init, unwrap = params_wrap(init_list)\n",
    "    def wrap_cost(vec, *args):\n",
    "        E, params_bar = cost(unwrap(vec), *args)\n",
    "        vec_bar, _ = params_wrap(params_bar)\n",
    "        return E, vec_bar\n",
    "    res = minimize(wrap_cost, init, args, 'L-BFGS-B', jac=True, options=opt)\n",
    "    return unwrap(res.x)\n",
    "\n",
    "\n",
    "def linreg_cost(params, X, yy, alpha):\n",
    "    \"\"\"Regularized least squares cost function and gradients\n",
    "\n",
    "    Can be optimized with minimize_list -- see fit_linreg_gradopt for a\n",
    "    demonstration.\n",
    "\n",
    "    Inputs:\n",
    "    params: tuple (ww, bb): weights ww (D,), bias bb scalar\n",
    "         X: N,D design matrix of input features\n",
    "        yy: N,  real-valued targets\n",
    "     alpha: regularization constant\n",
    "\n",
    "    Outputs: (E, [ww_bar, bb_bar]), cost and gradients\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb = params\n",
    "\n",
    "    # forward computation of error\n",
    "    ff = np.dot(X, ww) + bb\n",
    "    res = ff - yy\n",
    "    E = np.dot(res, res) + alpha*np.dot(ww, ww)\n",
    "\n",
    "    # reverse computation of gradients\n",
    "    ff_bar = 2*res\n",
    "    bb_bar = np.sum(ff_bar)\n",
    "    ww_bar = np.dot(X.T, ff_bar) + 2*alpha*ww\n",
    "\n",
    "    return E, [ww_bar, bb_bar]\n",
    "\n",
    "\n",
    "def fit_linreg_gradopt(X, yy, alpha):\n",
    "    \"\"\"\n",
    "    fit a regularized linear regression model with gradient opt\n",
    "\n",
    "         ww, bb = fit_linreg_gradopt(X, yy, alpha)\n",
    "\n",
    "     Find weights and bias by using a gradient-based optimizer\n",
    "     (minimize_list) to improve the regularized least squares cost:\n",
    "\n",
    "       np.sum(((np.dot(X,ww) + bb) - yy)**2) + alpha*np.dot(ww,ww)\n",
    "\n",
    "     Inputs:\n",
    "             X N,D design matrix of input features\n",
    "            yy N,  real-valued targets\n",
    "         alpha     scalar regularization constant\n",
    "\n",
    "     Outputs:\n",
    "            ww D,  fitted weights\n",
    "            bb     scalar fitted bias\n",
    "    \"\"\"\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    init = (np.zeros(D), np.array(0))\n",
    "    ww, bb = minimize_list(linreg_cost, init, args)\n",
    "    return ww, bb\n",
    "\n",
    "\n",
    "def logreg_cost(params, X, yy, alpha):\n",
    "    \"\"\"Regularized logistic regression cost function and gradients\n",
    "\n",
    "    Can be optimized with minimize_list -- see fit_linreg_gradopt for a\n",
    "    demonstration of fitting a similar function.\n",
    "\n",
    "    Inputs:\n",
    "    params: tuple (ww, bb): weights ww (D,), bias bb scalar\n",
    "         X: N,D design matrix of input features\n",
    "        yy: N,  real-valued targets\n",
    "     alpha: regularization constant\n",
    "\n",
    "    Outputs: (E, [ww_bar, bb_bar]), cost and gradients\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb = params\n",
    "\n",
    "    # Force targets to be +/- 1\n",
    "    yy = 2*(yy==1) - 1\n",
    "\n",
    "    # forward computation of error\n",
    "    aa = yy*(np.dot(X, ww) + bb)\n",
    "    sigma = 1/(1 + np.exp(-aa))\n",
    "    E = -np.sum(np.log(sigma)) + alpha*np.dot(ww, ww)\n",
    "\n",
    "    # reverse computation of gradients\n",
    "    aa_bar = sigma - 1\n",
    "    bb_bar = np.dot(aa_bar, yy)\n",
    "    ww_bar = np.dot(X.T, yy*aa_bar) + 2*alpha*ww\n",
    "\n",
    "    return E, (ww_bar, bb_bar)\n",
    "\n",
    "\n",
    "def nn_cost(params, X, yy=None, alpha=None):\n",
    "    \"\"\"NN_COST simple neural network cost function and gradients, or predictions\n",
    "\n",
    "           E, params_bar = nn_cost([ww, bb, V, bk], X, yy, alpha)\n",
    "                    pred = nn_cost([ww, bb, V, bk], X)\n",
    "\n",
    "     Cost function E can be minimized with minimize_list\n",
    "\n",
    "     Inputs:\n",
    "             params (ww, bb, V, bk), where:\n",
    "                    --------------------------------\n",
    "                        ww K,  hidden-output weights\n",
    "                        bb     scalar output bias\n",
    "                        V  K,D hidden-input weights\n",
    "                        bk K,  hidden biases\n",
    "                    --------------------------------\n",
    "                  X N,D input design matrix\n",
    "                 yy N,  regression targets\n",
    "              alpha     scalar regularization for weights\n",
    "\n",
    "     Outputs:\n",
    "                     E  sum of squares error\n",
    "            params_bar  gradients wrt params, same format as params\n",
    "     OR\n",
    "               pred N,  predictions if only params and X are given as inputs\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb, V, bk = params\n",
    "\n",
    "    # Forwards computation of cost\n",
    "    A = np.dot(X, V.T) + bk[None,:] # N,K\n",
    "    P = 1 / (1 + np.exp(-A)) # N,K\n",
    "    F = np.dot(P, ww) + bb # N,\n",
    "    if yy is None:\n",
    "        # user wants prediction rather than training signal:\n",
    "        return F\n",
    "    res = F - yy # N,\n",
    "    E = np.dot(res, res) + alpha*(np.sum(V*V) + np.dot(ww,ww)) # 1x1\n",
    "\n",
    "    # Reverse computation of gradients\n",
    "    F_bar = 2*res # N,\n",
    "    ww_bar = np.dot(P.T, F_bar) + 2*alpha*ww # K,\n",
    "    bb_bar = np.sum(F_bar) # scalar\n",
    "    P_bar = np.dot(F_bar[:,None], ww[None,:]) # N,K\n",
    "    A_bar = P_bar * P * (1 - P) # N,K\n",
    "    V_bar = np.dot(A_bar.T, X) + 2*alpha*V # K,D\n",
    "    bk_bar = np.sum(A_bar, 0)\n",
    "\n",
    "    return E, (ww_bar, bb_bar, V_bar, bk_bar)\n",
    "\n",
    "\n",
    "def rbf_fn(X1, X2):\n",
    "    \"\"\"Helper routine for gp_post_par\"\"\"\n",
    "    return np.exp((np.dot(X1,(2*X2.T))-np.sum(X1*X1,1)[:,None]) - np.sum(X2*X2,1)[None,:])\n",
    "\n",
    "\n",
    "def gauss_kernel_fn(X1, X2, ell, sigma_f):\n",
    "    \"\"\"Helper routine for gp_post_par\"\"\"\n",
    "    return sigma_f**2 * rbf_fn(X1/(np.sqrt(2)*ell), X2/(np.sqrt(2)*ell))\n",
    "\n",
    "\n",
    "def gp_post_par(X_rest, X_obs, yy, sigma_y=0.05, ell=5.0, sigma_f=0.1):\n",
    "    \"\"\"GP_POST_PAR means and covariances of a posterior Gaussian process\n",
    "\n",
    "         rest_cond_mu, rest_cond_cov = gp_post_par(X_rest, X_obs, yy)\n",
    "         rest_cond_mu, rest_cond_cov = gp_post_par(X_rest, X_obs, yy, sigma_y, ell, sigma_f)\n",
    "\n",
    "     Calculate the means and covariances at all test locations of the posterior Gaussian\n",
    "     process conditioned on the observations yy at observed locations X_obs.\n",
    "\n",
    "     Inputs:\n",
    "                 X_rest GP test locations\n",
    "                  X_obs locations of observations\n",
    "                     yy observed values\n",
    "                sigma_y observation noise standard deviation\n",
    "                    ell kernel function length scale\n",
    "                sigma_f kernel function standard deviation\n",
    "\n",
    "     Outputs:\n",
    "           rest_cond_mu mean at each location in X_rest\n",
    "          rest_cond_cov covariance matrix between function values at all test locations\n",
    "    \"\"\"\n",
    "    X_rest = X_rest[:, None]\n",
    "    X_obs = X_obs[:, None]\n",
    "    K_rest = gauss_kernel_fn(X_rest, X_rest, ell, sigma_f)\n",
    "    K_rest_obs = gauss_kernel_fn(X_rest, X_obs, ell, sigma_f)\n",
    "    K_obs = gauss_kernel_fn(X_obs, X_obs, ell, sigma_f)\n",
    "    M = K_obs + sigma_y**2 * np.eye(yy.size)\n",
    "    M_cho, M_low = cho_factor(M)\n",
    "    rest_cond_mu = np.dot(K_rest_obs, cho_solve((M_cho, M_low), yy))\n",
    "    rest_cond_cov = K_rest - np.dot(K_rest_obs, cho_solve((M_cho, M_low), K_rest_obs.T))\n",
    "\n",
    "    return rest_cond_mu, rest_cond_cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./ct_data.npz')\n",
    "X_train = data['X_train']; X_val = data['X_val']; X_test = data['X_test']\n",
    "y_train = data['y_train']; y_val = data['y_val']; y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the training positions in y_train is -0.0000 and the standard error of the mean is 0.0050.\n",
      "The mean of the 5,785 positions in the y_val is -0.2160 and the standard error of the mean is  0.0129.\n",
      "The mean of the first 5,785 entries in the y_train is -0.4425 and the standard error of the mean is  0.0119.\n"
     ]
    }
   ],
   "source": [
    "print(f'The mean of the training positions in y_train is {np.mean(y_train):.4f} and the standard error of the mean is {np.std(y_train)/np.sqrt(len(y_train)):.4f}.')\n",
    "print(f'The mean of the 5,785 positions in the y_val is {np.mean(y_val):.4f} and the standard error of the mean is  {np.std(y_val)/np.sqrt(len(y_val)):.4f}.')\n",
    "print(f'The mean of the first 5,785 entries in the y_train is {np.mean(y_train[0:5785,]):.4f} and the standard error of the mean is  {np.std(y_train[0:5785,])/np.sqrt(5785):.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain how your results demonstrate that these standard error bars do not reliably indicate what the average of locations in future CT slice data will be. Why are standard error bars misleading here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ni hao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column indexes of constants: [ 59  69 179 189 351].\n",
      "Column indexes of dupulicates: [ 69  78  79 179 188 189 199 287 351 359].\n",
      "Column indexes for removal: [ 59  69  78  79 179 188 189 199 287 351 359].\n"
     ]
    }
   ],
   "source": [
    "# stage one: constant columns\n",
    "index1 = np.where(np.var(X_train, axis=0) == 0)[0]\n",
    "print(f'Column indexes of constants: {index1}.')\n",
    "\n",
    "# stage two: identical columns\n",
    "index2 = np.delete(np.arange(0, X_train.shape[1]), np.sort(np.unique(X_train, axis=1, return_index=True)[1]))\n",
    "print(f'Column indexes of dupulicates: {index2}.')\n",
    "\n",
    "# indeices for removal\n",
    "index_remove = np.unique(np.concatenate((index1, index2), axis=0))\n",
    "print(f'Column indexes for removal: {index_remove}.')\n",
    "\n",
    "# removal\n",
    "X_train = np.delete(X_train, index_remove, axis=1)\n",
    "X_val = np.delete(X_val, index_remove, axis=1)\n",
    "X_test = np.delete(X_test, index_remove, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report which columns of the X_... arrays you remove at each of the two stages. Report these as 0-based indexes. (For the second stage, you might report indexes in the original array, or after you did the first stage. It doesn’t matter, as long as your code is clear and correct.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linreg(X, yy, alpha):\n",
    "    n, d = X.shape[0], X.shape[1]\n",
    "    y_tilde = np.concatenate((yy, np.zeros(d)))\n",
    "    Phi = np.concatenate((X, np.ones((n, 1))), axis=1)\n",
    "    diag = np.diag(np.append(np.sqrt(alpha) * np.ones((d)), [0]))\n",
    "    Phi_tilde = np.concatenate((Phi, diag))\n",
    "    y_tilde = np.concatenate((yy, np.zeros((d+1))))\n",
    "    w_tilde = np.linalg.lstsq(Phi_tilde, y_tilde, rcond=None)[0]\n",
    "    ww, bb = w_tilde[0:-1], w_tilde[-1]\n",
    "    return ww, bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit X_train and y_train with alpha = 30\n",
    "w_ls, b_ls = fit_linreg(X=X_train, yy=y_train, alpha=30)\n",
    "para_ls = np.concatenate((w_ls, b_ls), axis=None)\n",
    "pred_ls = np.concatenate((X_train, np.ones((len(X_train), 1))), axis=1) @ para_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_linreg_gradopt(X, yy, 30)\n",
    "w_gd, b_gd = fit_linreg_gradopt(X=X_train, yy=y_train, alpha=30)\n",
    "para_gd = np.concatenate((w_ls, b_ls), axis=None)\n",
    "pred_gd = np.concatenate((X_train, np.ones((len(X_train), 1))), axis=1) @ para_gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE using least square method is 0.3568.\n",
      "The RMSE using gradient descent method is 0.3568.\n"
     ]
    }
   ],
   "source": [
    "# Report RMSE = sqrt(||y - f(x;w,b)||^2 / N)\n",
    "RMSE_ls = np.sqrt(np.mean((y_train - pred_ls) ** 2))\n",
    "RMSE_gd = np.sqrt(np.mean((y_train - pred_gd) ** 2))\n",
    "print(f'The RMSE using least square method is {RMSE_ls:.4f}.')\n",
    "print(f'The RMSE using gradient descent method is {RMSE_gd:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logreg_gradopt(X, yy, alpha):\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    init = (np.zeros(D), np.array(0))\n",
    "    ww, bb = minimize_list(logreg_cost, init, args)\n",
    "    return ww, bb\n",
    "\n",
    "def logreg_prob(X, ww, bb):\n",
    "    return 1 / (1 + np.exp(- (X @ ww + bb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20 # number of thresholded classification problems to fit\n",
    "mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)\n",
    "prob_train = np.zeros((len(X_train), K))\n",
    "prob_val = np.zeros((len(X_val), K))\n",
    "para_w = np.zeros((X_train.shape[1], K))\n",
    "para_b = np.zeros(K)\n",
    "\n",
    "for kk in range(K):\n",
    "    labels = (y_train > thresholds[kk]).astype(int)\n",
    "    w_logr, b_logr = fit_logreg_gradopt(X=X_train, yy=labels, alpha=30)\n",
    "    para_w[:,kk] = w_logr\n",
    "    para_b[kk] = b_logr\n",
    "    prob_train[:,kk] = logreg_prob(X=X_train, ww=w_logr, bb=b_logr)\n",
    "    prob_val[:,kk] = logreg_prob(X=X_val, ww=w_logr, bb=b_logr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit regularised linear regression based on the results of logistics regression\n",
    "# least square\n",
    "w_ls_log, b_ls_log = fit_linreg(X=prob_train, yy=y_train, alpha=30)\n",
    "para_ls_log = np.concatenate((w_ls_log, b_ls_log), axis=None)\n",
    "pred_ls_log = np.concatenate((prob_train, np.ones((len(prob_train), 1))), axis=1) @ para_ls_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "w_gd_log, b_gd_log = fit_linreg_gradopt(X=prob_train, yy=y_train, alpha=30)\n",
    "para_gd_log = np.concatenate((w_gd_log, b_gd_log), axis=None)\n",
    "pred_gd_log = np.concatenate((prob_train, np.ones((len(prob_train), 1))), axis=1) @ para_gd_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using the TRAIN set!\n",
      "The RMSE using least square method is 0.1544.\n",
      "The RMSE using gradient descent method is 0.1544.\n"
     ]
    }
   ],
   "source": [
    "# Report RMSE = sqrt(||y - f(x;w,b)||^2 / N)\n",
    "# This is for train set\n",
    "RMSE_ls_log_train = np.sqrt(np.mean((y_train - pred_ls_log) ** 2))\n",
    "RMSE_gd_log_train = np.sqrt(np.mean((y_train - pred_gd_log) ** 2))\n",
    "\n",
    "print('We are using the TRAIN set!')\n",
    "print(f'The RMSE using least square method is {RMSE_ls_log_train:.4f}.')\n",
    "print(f'The RMSE using gradient descent method is {RMSE_gd_log_train:.4f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using the VAL set!\n",
      "The RMSE using least square method is 0.2542.\n",
      "The RMSE using gradient descent method is 0.2542.\n"
     ]
    }
   ],
   "source": [
    "# Report RMSE = sqrt(||y - f(x;w,b)||^2 / N)\n",
    "# This is for val set\n",
    "pred_ls_log_val = np.concatenate((prob_val, np.ones((len(prob_val), 1))), axis=1) @ para_ls_log\n",
    "pred_gd_log_val = np.concatenate((prob_val, np.ones((len(prob_val), 1))), axis=1) @ para_gd_log\n",
    "\n",
    "RMSE_ls_log_val = np.sqrt(np.mean((y_val - pred_ls_log_val) ** 2))\n",
    "RMSE_gd_log_val = np.sqrt(np.mean((y_val - pred_gd_log_val) ** 2))\n",
    "\n",
    "print('We are using the VAL set!')\n",
    "print(f'The RMSE using least square method is {RMSE_ls_log_val:.4f}.')\n",
    "print(f'The RMSE using gradient descent method is {RMSE_gd_log_val:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using the TRAIN set!\n",
      "The RMSE of nn with random initialisaiton is 0.1403.\n"
     ]
    }
   ],
   "source": [
    "# minimise nn cost using minimize_list with random intialisation from lecture note\n",
    "\n",
    "# random initialisation\n",
    "d, k = X_train.shape[1], 20\n",
    "ww = 0.1*np.random.randn(k)/np.sqrt(k)\n",
    "bb = 0.1*np.random.randn()/np.sqrt(k)\n",
    "V = 0.1*np.random.randn(k, d)/np.sqrt(k)\n",
    "bk = 0.1*np.random.randn(k)/np.sqrt(k)\n",
    "nn_init = (ww, bb, V, bk)\n",
    "\n",
    "# X_train, y_train and alpha\n",
    "nn_alpha = 30\n",
    "nn_args = (X_train, y_train, nn_alpha)\n",
    "\n",
    "params_random = minimize_list(nn_cost, nn_init, nn_args)\n",
    "\n",
    "pred_nn_random_train = nn_cost(params_random, X=X_train)\n",
    "RMSE_nn_random_train = np.sqrt(np.mean((y_train - pred_nn_random_train) ** 2))\n",
    "\n",
    "print('We are using the TRAIN set!')\n",
    "print(f'The RMSE of nn with random initialisaiton is {RMSE_nn_random_train:.4f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using the TRAIN set!\n",
      "The RMSE of nn with Q3 initialisaiton is 0.1396.\n"
     ]
    }
   ],
   "source": [
    "# minimise nn cost using minimize_list with Q3 intialisation\n",
    "\n",
    "# Q3 initialisation\n",
    "nn_init_q3 = (w_ls_log, b_ls_log, para_w.T, para_b)\n",
    "\n",
    "# X_train, y_train and alpha\n",
    "nn_alpha = 30\n",
    "nn_args = (X_train, y_train, nn_alpha)\n",
    "\n",
    "params_q3 = minimize_list(nn_cost, nn_init_q3, nn_args)\n",
    "\n",
    "pred_nn_q3_train = nn_cost(params_q3, X=X_train)\n",
    "RMSE_nn_q3_train = np.sqrt(np.mean((y_train - pred_nn_q3_train) ** 2))\n",
    "\n",
    "print('We are using the TRAIN set!')\n",
    "print(f'The RMSE of nn with Q3 initialisaiton is {RMSE_nn_q3_train:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayesian optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_reg(XX, XX_val, yy, yy_val, params, alpha, report_train=False, report_val=True):\n",
    "    args = (XX_val, yy_val, alpha)\n",
    "    params_bar = minimize_list(nn_cost, params, args)\n",
    "    pred = nn_cost(params_bar, X=XX)\n",
    "    RMSE = np.sqrt(np.mean((yy - pred) ** 2))\n",
    "    pred_val = nn_cost(params_bar, X=XX_val)\n",
    "    RMSE_val = np.sqrt(np.mean((yy_val - pred_val) ** 2))\n",
    "    if report_train:\n",
    "        print(f'The RMSE_train of nn with Q3 initialisaiton is {RMSE:.4f} while alpha is {alpha}.')\n",
    "    if report_val:\n",
    "        print(f'The RMSE_val of nn with Q3 initialisaiton is {RMSE_val:.4f} while alpha is {alpha}.')\n",
    "    return RMSE_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE_val of nn with Q3 initialisaiton is 0.1767 while alpha is 30.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.1058 while alpha is 8.76.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.1044 while alpha is 8.46.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.1252 while alpha is 13.280000000000001.\n",
      "Iteration 1, the maximal PI is 0.03131035687909651 with alpha equals to 8.68.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.1373 while alpha is 16.72.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.1840 while alpha is 33.06.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.1453 while alpha is 19.14.\n",
      "Iteration 2, the maximal PI is 0.13225510069064217 with alpha equals to 15.66.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.1535 while alpha is 21.92.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.2049 while alpha is 42.88.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.0816 while alpha is 4.24.\n",
      "Iteration 3, the maximal PI is 0.00027779653371804564 with alpha equals to 4.68.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.1172 while alpha is 10.120000000000001.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.1844 while alpha is 33.34.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.1538 while alpha is 22.02.\n",
      "Iteration 4, the maximal PI is 0.04071114685198162 with alpha equals to 11.620000000000001.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.1849 while alpha is 33.52.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.1227 while alpha is 11.42.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.0906 while alpha is 5.82.\n",
      "Iteration 5, the maximal PI is 0.001955067011846108 with alpha equals to 5.92.\n"
     ]
    }
   ],
   "source": [
    "# initialation\n",
    "alpha_set = np.arange(0, 50, 0.02)\n",
    "params_q3 = (w_ls_log, b_ls_log, para_w.T, para_b)\n",
    "points_num = 3\n",
    "RMSE_alpha_base = train_nn_reg(XX=X_train, XX_val=X_val, yy=y_train, yy_val=y_val, params=params_q3, alpha=30)\n",
    "\n",
    "for i in range(5):\n",
    "    pick = np.random.randint(0, len(alpha_set), points_num)\n",
    "    alpha_train = alpha_set[pick]\n",
    "    alpha_acquisition = np.delete(alpha_set, pick)\n",
    "    RMSE_val = np.zeros(points_num)\n",
    "    for j in range(points_num):\n",
    "        RMSE_val[j] = train_nn_reg(XX=X_train, XX_val=X_val, yy=y_train, yy_val=y_val, params=params_q3, alpha=alpha_train[j])\n",
    "    y = np.log(RMSE_alpha_base) - np.log(RMSE_val)\n",
    "    mu, cov = gp_post_par(X_rest=alpha_acquisition, X_obs=alpha_train, yy=y)\n",
    "    sigma = np.sqrt(np.diag(cov))\n",
    "    PI = scipy.stats.norm.cdf((mu - y.max()) / sigma)\n",
    "    print(f'Iteration {i+1}, the maximal PI is {PI.max()} with alpha equals to {alpha_acquisition[np.argmax(PI)]}.')\n",
    "    alpha_set = np.append(alpha_set, alpha_acquisition[np.argmax(PI)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE_val of nn with Q3 initialisaiton is 0.4034 while alpha is 5.92.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.0911 while alpha is 5.92.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09107310843522953"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nn_reg(XX=X_train, XX_val=X_val, yy=y_train, yy_val=y_val, \n",
    "             params=params_q3, alpha=alpha_acquisition[np.argmax(PI)], report_train=True, report_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using the TRAIN set!\n",
      "The RMSE of nn with Q3 initialisaiton is 0.0781.\n"
     ]
    }
   ],
   "source": [
    "# Q3 initialisation\n",
    "nn_init_q3 = (w_ls_log, b_ls_log, para_w.T, para_b)\n",
    "\n",
    "# X_train, y_train and alpha\n",
    "nn_alpha = alpha_acquisition[np.argmax(PI)]\n",
    "nn_args = (X_train, y_train, nn_alpha)\n",
    "\n",
    "params_q3 = minimize_list(nn_cost, nn_init_q3, nn_args)\n",
    "\n",
    "pred_nn_q5_train = nn_cost(params_q3, X=X_train)\n",
    "RMSE_nn_q5_train = np.sqrt(np.mean((y_train - pred_nn_q5_train) ** 2))\n",
    "\n",
    "print('We are using the TRAIN set!')\n",
    "print(f'The RMSE of nn with Q3 initialisaiton is {RMSE_nn_q5_train:.4f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nn_q5_val = nn_cost(params_q3, X=X_val)\n",
    "RMSE_nn_q5_val = np.sqrt(np.mean((y_val - pred_nn_q5_val) ** 2))\n",
    "\n",
    "print('We are using the VAL set!')\n",
    "print(f'The RMSE of nn with Q3 initialisaiton is {RMSE_nn_q5_val:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA then perform the regression and nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n",
      "It should be 372.\n",
      "373\n"
     ]
    }
   ],
   "source": [
    "# notice that column 271 is actually constant only one data set with a different value\n",
    "X_train_no271 = np.delete(X_train, 271, axis=1)\n",
    "X_val_no271 = np.delete(X_val, 271, axis=1)\n",
    "print(X_train_no271.shape[1])\n",
    "print(f'It should be {X_train.shape[1]-1}.')\n",
    "\n",
    "# standardisation\n",
    "X_train_std = (X_train_no271 - X_train_no271.mean(axis=0)) / X_train_no271.std(axis=0)\n",
    "X_val_std = (X_val_no271 - X_val_no271.mean(axis=0)) / X_val_no271.std(axis=0)\n",
    "\n",
    "# PCA calculation\n",
    "X_train_std_cov = np.cov(X_train_std, ddof=1, rowvar=False)\n",
    "X_train_std_cov_eigval, X_train_std_cov_eigvec = np.linalg.eig(X_train_std_cov)\n",
    "PC_order = np.argsort(X_train_std_cov_eigval)[::-1]\n",
    "X_train_std_cov_eigval = X_train_std_cov_eigval[PC_order]\n",
    "X_train_std_cov_eigvec = X_train_std_cov_eigvec[:,PC_order]\n",
    "explained_variance = X_train_std_cov_eigval / np.sum(X_train_std_cov_eigval)\n",
    "accum_explained_variance = np.add.accumulate(explained_variance)\n",
    "\n",
    "# retain 95% or try other of information\n",
    "pc = sum(accum_explained_variance <= 1) + 1\n",
    "print(pc)\n",
    "\n",
    "# PCA\n",
    "X_train_pca = np.matmul(X_train_std, X_train_std_cov_eigvec[:,:pc])\n",
    "X_val_pca = np.matmul(X_val_std, X_train_std_cov_eigvec[:,:pc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using the TRAIN set!\n",
      "The RMSE using least square method is 0.0903.\n",
      "We are using the VAL set!\n",
      "The RMSE using least square method is 0.2331.\n"
     ]
    }
   ],
   "source": [
    "K = 20 # number of thresholded classification problems to fit\n",
    "mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)\n",
    "prob_train_pca = np.zeros((len(X_train_pca), K))\n",
    "prob_val_pca = np.zeros((len(X_val_pca), K))\n",
    "para_w_pca = np.zeros((X_train_pca.shape[1], K))\n",
    "para_b_pca = np.zeros(K)\n",
    "\n",
    "for kk in range(K):\n",
    "    labels = (y_train > thresholds[kk]).astype(int)\n",
    "    w_logr_pca, b_logr_pca = fit_logreg_gradopt(X=X_train_pca, yy=labels, alpha=30)\n",
    "    para_w_pca[:,kk] = w_logr_pca\n",
    "    para_b_pca[kk] = b_logr_pca\n",
    "    prob_train_pca[:,kk] = logreg_prob(X=X_train_pca, ww=w_logr_pca, bb=b_logr_pca)\n",
    "    prob_val_pca[:,kk] = logreg_prob(X=X_val_pca, ww=w_logr_pca, bb=b_logr_pca)\n",
    "\n",
    "w_ls_log_pca, b_ls_log_pca = fit_linreg(X=prob_train_pca, yy=y_train, alpha=30)\n",
    "para_ls_log_pca = np.concatenate((w_ls_log_pca, b_ls_log_pca), axis=None)\n",
    "pred_ls_log_pca = np.concatenate((prob_train_pca, np.ones((len(prob_train_pca), 1))), axis=1) @ para_ls_log_pca\n",
    "RMSE_ls_log_train_pca = np.sqrt(np.mean((y_train - pred_ls_log_pca) ** 2))\n",
    "pred_ls_log_val_pca = np.concatenate((prob_val_pca, np.ones((len(prob_val_pca), 1))), axis=1) @ para_ls_log_pca\n",
    "RMSE_ls_log_val_pca = np.sqrt(np.mean((y_val - pred_ls_log_val_pca) ** 2))\n",
    "\n",
    "print('We are using the TRAIN set!')\n",
    "print(f'The RMSE using least square method is {RMSE_ls_log_train_pca:.4f}.')\n",
    "print('We are using the VAL set!')\n",
    "print(f'The RMSE using least square method is {RMSE_ls_log_val_pca:.4f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using the TRAIN set!\n",
      "The RMSE of nn with Q3 initialisaiton is 0.0870.\n"
     ]
    }
   ],
   "source": [
    "# minimise nn cost using minimize_list with Q3 intialisation\n",
    "\n",
    "# Q3 initialisation\n",
    "nn_init_q6_pca = (w_ls_log_pca, b_ls_log_pca, para_w_pca.T, para_b_pca)\n",
    "\n",
    "# X_train, y_train and alpha\n",
    "nn_alpha = 30\n",
    "nn_args_pca = (X_train_pca, y_train, nn_alpha)\n",
    "\n",
    "params_q6_pca = minimize_list(nn_cost, nn_init_q6_pca, nn_args_pca)\n",
    "\n",
    "pred_nn_q6_train_pca = nn_cost(params_q6_pca, X=X_train_pca)\n",
    "RMSE_nn_q6_train_pca = np.sqrt(np.mean((y_train - pred_nn_q6_train_pca) ** 2))\n",
    "\n",
    "print('We are using the TRAIN set!')\n",
    "print(f'The RMSE of nn with Q3 initialisaiton is {RMSE_nn_q6_train_pca:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some change made for below chunk. (Please verify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE_val of nn with Q3 initialisaiton is 0.0921 while alpha is 30.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.0751 while alpha is 17.76.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.0817 while alpha is 21.5.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.1022 while alpha is 38.7.\n",
      "Iteration 1, the maximal PI is 0.1789962163309276 with alpha equals to 16.54.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.1060 while alpha is 42.28.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.0652 while alpha is 12.72.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.0483 while alpha is 7.08.\n",
      "Iteration 2, the maximal PI is 0.0026004882388388716 with alpha equals to 7.12.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.0863 while alpha is 25.32.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.1047 while alpha is 40.92.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.0893 while alpha is 27.68.\n",
      "Iteration 3, the maximal PI is 0.38293668369629297 with alpha equals to 22.96.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.1029 while alpha is 39.4.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.0990 while alpha is 34.44.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.0708 while alpha is 15.620000000000001.\n",
      "Iteration 4, the maximal PI is 0.1200992827623189 with alpha equals to 15.38.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.0368 while alpha is 3.36.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.0648 while alpha is 12.56.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.1090 while alpha is 45.6.\n",
      "Iteration 5, the maximal PI is 8.618366576063455e-05 with alpha equals to 4.78.\n"
     ]
    }
   ],
   "source": [
    "# initialation\n",
    "alpha_set = np.arange(0, 50, 0.02)\n",
    "nn_init_q6_pca = (w_ls_log_pca, b_ls_log_pca, para_w_pca.T, para_b_pca)\n",
    "points_num = 3\n",
    "RMSE_alpha_base_pca = train_nn_reg(XX=X_train_pca, XX_val=X_val_pca, yy=y_train, yy_val=y_val, params=nn_init_q6_pca, alpha=30)\n",
    "\n",
    "for i in range(5):\n",
    "    pick = np.random.randint(0, len(alpha_set), points_num)\n",
    "    alpha_train = alpha_set[pick]\n",
    "    alpha_acquisition = np.delete(alpha_set, pick)\n",
    "    RMSE_val = np.zeros(points_num)\n",
    "    for j in range(points_num):\n",
    "        RMSE_val[j] = train_nn_reg(XX=X_train_pca, XX_val=X_val_pca, yy=y_train, yy_val=y_val, params=nn_init_q6_pca, alpha=alpha_train[j])\n",
    "    y = np.log(RMSE_alpha_base_pca) - np.log(RMSE_val)\n",
    "    mu, cov = gp_post_par(X_rest=alpha_acquisition, X_obs=alpha_train, yy=y)\n",
    "    sigma = np.sqrt(np.diag(cov))\n",
    "    PI = scipy.stats.norm.cdf((mu - y.max()) / sigma)\n",
    "    print(f'Iteration {i+1}, the maximal PI is {PI.max()} with alpha equals to {alpha_acquisition[np.argmax(PI)]}.')\n",
    "    RMSE_alpha_base_pca = train_nn_reg(XX=X_train_pca, XX_val=X_val_pca, yy=y_train, yy_val=y_val, params=nn_init_q6_pca, alpha=alpha_acquisition[np.argmax(PI)])\n",
    "    alpha_set = np.append(alpha_set, alpha_acquisition[np.argmax(PI)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE_train of nn with Q3 initialisaiton is 0.4288 while alpha is 4.78.\n",
      "The RMSE_val of nn with Q3 initialisaiton is 0.0431 while alpha is 4.78.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04309593005138647"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nn_reg(XX=X_train_pca, XX_val=X_val_pca, yy=y_train, yy_val=y_val, \n",
    "             params=nn_init_q6_pca, alpha=alpha_acquisition[np.argmax(PI)], report_train=True, report_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using the TRAIN set!\n",
      "The RMSE of nn with Q3 initialisaiton is 0.0500.\n"
     ]
    }
   ],
   "source": [
    "# X_train, y_train and alpha\n",
    "nn_alpha = alpha_acquisition[np.argmax(PI)]\n",
    "nn_alpha = 0\n",
    "nn_args_pca = (X_train_pca, y_train, nn_alpha)\n",
    "\n",
    "params_q6_pca = minimize_list(nn_cost, nn_init_q6_pca, nn_args_pca)\n",
    "\n",
    "pred_nn_q6_train_pca = nn_cost(params_q6_pca, X=X_train_pca)\n",
    "RMSE_nn_q6_train_pca = np.sqrt(np.mean((y_train - pred_nn_q6_train_pca) ** 2))\n",
    "\n",
    "print('We are using the TRAIN set!')\n",
    "print(f'The RMSE of nn with Q3 initialisaiton is {RMSE_nn_q6_train_pca:.4f}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
