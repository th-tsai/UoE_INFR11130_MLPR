{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import cho_factor, cho_solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_unwrap(param_vec, shapes, sizes):\n",
    "    \"\"\"Helper routine for minimize_list\"\"\"\n",
    "    args = []\n",
    "    pos = 0\n",
    "    for i in range(len(shapes)):\n",
    "        sz = sizes[i]\n",
    "        args.append(param_vec[pos:pos+sz].reshape(shapes[i]))\n",
    "        pos += sz\n",
    "    return args\n",
    "\n",
    "\n",
    "def params_wrap(param_list):\n",
    "    \"\"\"Helper routine for minimize_list\"\"\"\n",
    "    param_list = [np.array(x) for x in param_list]\n",
    "    shapes = [x.shape for x in param_list]\n",
    "    sizes = [x.size for x in param_list]\n",
    "    param_vec = np.zeros(sum(sizes))\n",
    "    pos = 0\n",
    "    for param in param_list:\n",
    "        sz = param.size\n",
    "        param_vec[pos:pos+sz] = param.ravel()\n",
    "        pos += sz\n",
    "    unwrap = lambda pvec: params_unwrap(pvec, shapes, sizes)\n",
    "    return param_vec, unwrap\n",
    "\n",
    "\n",
    "def minimize_list(cost, init_list, args):\n",
    "    \"\"\"Optimize a list of arrays (wrapper of scipy.optimize.minimize)\n",
    "\n",
    "    The input function \"cost\" should take a list of parameters,\n",
    "    followed by any extra arguments:\n",
    "        cost(init_list, *args)\n",
    "    should return the cost of the initial condition, and a list in the same\n",
    "    format as init_list giving gradients of the cost wrt the parameters.\n",
    "\n",
    "    The options to the optimizer have been hard-coded. You may wish\n",
    "    to change disp to True to get more diagnostics. You may want to\n",
    "    decrease maxiter while debugging. Although please report all results\n",
    "    in Q2-5 using maxiter=500.\n",
    "    \"\"\"\n",
    "    opt = {'maxiter': 500, 'disp': False}\n",
    "    init, unwrap = params_wrap(init_list)\n",
    "    def wrap_cost(vec, *args):\n",
    "        E, params_bar = cost(unwrap(vec), *args)\n",
    "        vec_bar, _ = params_wrap(params_bar)\n",
    "        return E, vec_bar\n",
    "    res = minimize(wrap_cost, init, args, 'L-BFGS-B', jac=True, options=opt)\n",
    "    return unwrap(res.x)\n",
    "\n",
    "\n",
    "def linreg_cost(params, X, yy, alpha):\n",
    "    \"\"\"Regularized least squares cost function and gradients\n",
    "\n",
    "    Can be optimized with minimize_list -- see fit_linreg_gradopt for a\n",
    "    demonstration.\n",
    "\n",
    "    Inputs:\n",
    "    params: tuple (ww, bb): weights ww (D,), bias bb scalar\n",
    "         X: N,D design matrix of input features\n",
    "        yy: N,  real-valued targets\n",
    "     alpha: regularization constant\n",
    "\n",
    "    Outputs: (E, [ww_bar, bb_bar]), cost and gradients\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb = params\n",
    "\n",
    "    # forward computation of error\n",
    "    ff = np.dot(X, ww) + bb\n",
    "    res = ff - yy\n",
    "    E = np.dot(res, res) + alpha*np.dot(ww, ww)\n",
    "\n",
    "    # reverse computation of gradients\n",
    "    ff_bar = 2*res\n",
    "    bb_bar = np.sum(ff_bar)\n",
    "    ww_bar = np.dot(X.T, ff_bar) + 2*alpha*ww\n",
    "\n",
    "    return E, [ww_bar, bb_bar]\n",
    "\n",
    "\n",
    "def fit_linreg_gradopt(X, yy, alpha):\n",
    "    \"\"\"\n",
    "    fit a regularized linear regression model with gradient opt\n",
    "\n",
    "         ww, bb = fit_linreg_gradopt(X, yy, alpha)\n",
    "\n",
    "     Find weights and bias by using a gradient-based optimizer\n",
    "     (minimize_list) to improve the regularized least squares cost:\n",
    "\n",
    "       np.sum(((np.dot(X,ww) + bb) - yy)**2) + alpha*np.dot(ww,ww)\n",
    "\n",
    "     Inputs:\n",
    "             X N,D design matrix of input features\n",
    "            yy N,  real-valued targets\n",
    "         alpha     scalar regularization constant\n",
    "\n",
    "     Outputs:\n",
    "            ww D,  fitted weights\n",
    "            bb     scalar fitted bias\n",
    "    \"\"\"\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    init = (np.zeros(D), np.array(0))\n",
    "    ww, bb = minimize_list(linreg_cost, init, args)\n",
    "    return ww, bb\n",
    "\n",
    "\n",
    "def logreg_cost(params, X, yy, alpha):\n",
    "    \"\"\"Regularized logistic regression cost function and gradients\n",
    "\n",
    "    Can be optimized with minimize_list -- see fit_linreg_gradopt for a\n",
    "    demonstration of fitting a similar function.\n",
    "\n",
    "    Inputs:\n",
    "    params: tuple (ww, bb): weights ww (D,), bias bb scalar\n",
    "         X: N,D design matrix of input features\n",
    "        yy: N,  real-valued targets\n",
    "     alpha: regularization constant\n",
    "\n",
    "    Outputs: (E, [ww_bar, bb_bar]), cost and gradients\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb = params\n",
    "\n",
    "    # Force targets to be +/- 1\n",
    "    yy = 2*(yy==1) - 1\n",
    "\n",
    "    # forward computation of error\n",
    "    aa = yy*(np.dot(X, ww) + bb)\n",
    "    sigma = 1/(1 + np.exp(-aa))\n",
    "    E = -np.sum(np.log(sigma)) + alpha*np.dot(ww, ww)\n",
    "\n",
    "    # reverse computation of gradients\n",
    "    aa_bar = sigma - 1\n",
    "    bb_bar = np.dot(aa_bar, yy)\n",
    "    ww_bar = np.dot(X.T, yy*aa_bar) + 2*alpha*ww\n",
    "\n",
    "    return E, (ww_bar, bb_bar)\n",
    "\n",
    "\n",
    "def nn_cost(params, X, yy=None, alpha=None):\n",
    "    \"\"\"NN_COST simple neural network cost function and gradients, or predictions\n",
    "\n",
    "           E, params_bar = nn_cost([ww, bb, V, bk], X, yy, alpha)\n",
    "                    pred = nn_cost([ww, bb, V, bk], X)\n",
    "\n",
    "     Cost function E can be minimized with minimize_list\n",
    "\n",
    "     Inputs:\n",
    "             params (ww, bb, V, bk), where:\n",
    "                    --------------------------------\n",
    "                        ww K,  hidden-output weights\n",
    "                        bb     scalar output bias\n",
    "                        V  K,D hidden-input weights\n",
    "                        bk K,  hidden biases\n",
    "                    --------------------------------\n",
    "                  X N,D input design matrix\n",
    "                 yy N,  regression targets\n",
    "              alpha     scalar regularization for weights\n",
    "\n",
    "     Outputs:\n",
    "                     E  sum of squares error\n",
    "            params_bar  gradients wrt params, same format as params\n",
    "     OR\n",
    "               pred N,  predictions if only params and X are given as inputs\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb, V, bk = params\n",
    "\n",
    "    # Forwards computation of cost\n",
    "    A = np.dot(X, V.T) + bk[None,:] # N,K\n",
    "    P = 1 / (1 + np.exp(-A)) # N,K\n",
    "    F = np.dot(P, ww) + bb # N,\n",
    "    if yy is None:\n",
    "        # user wants prediction rather than training signal:\n",
    "        return F\n",
    "    res = F - yy # N,\n",
    "    E = np.dot(res, res) + alpha*(np.sum(V*V) + np.dot(ww,ww)) # 1x1\n",
    "\n",
    "    # Reverse computation of gradients\n",
    "    F_bar = 2*res # N,\n",
    "    ww_bar = np.dot(P.T, F_bar) + 2*alpha*ww # K,\n",
    "    bb_bar = np.sum(F_bar) # scalar\n",
    "    P_bar = np.dot(F_bar[:,None], ww[None,:]) # N,K\n",
    "    A_bar = P_bar * P * (1 - P) # N,K\n",
    "    V_bar = np.dot(A_bar.T, X) + 2*alpha*V # K,D\n",
    "    bk_bar = np.sum(A_bar, 0)\n",
    "\n",
    "    return E, (ww_bar, bb_bar, V_bar, bk_bar)\n",
    "\n",
    "\n",
    "def rbf_fn(X1, X2):\n",
    "    \"\"\"Helper routine for gp_post_par\"\"\"\n",
    "    return np.exp((np.dot(X1,(2*X2.T))-np.sum(X1*X1,1)[:,None]) - np.sum(X2*X2,1)[None,:])\n",
    "\n",
    "\n",
    "def gauss_kernel_fn(X1, X2, ell, sigma_f):\n",
    "    \"\"\"Helper routine for gp_post_par\"\"\"\n",
    "    return sigma_f**2 * rbf_fn(X1/(np.sqrt(2)*ell), X2/(np.sqrt(2)*ell))\n",
    "\n",
    "\n",
    "def gp_post_par(X_rest, X_obs, yy, sigma_y=0.05, ell=5.0, sigma_f=0.1):\n",
    "    \"\"\"GP_POST_PAR means and covariances of a posterior Gaussian process\n",
    "\n",
    "         rest_cond_mu, rest_cond_cov = gp_post_par(X_rest, X_obs, yy)\n",
    "         rest_cond_mu, rest_cond_cov = gp_post_par(X_rest, X_obs, yy, sigma_y, ell, sigma_f)\n",
    "\n",
    "     Calculate the means and covariances at all test locations of the posterior Gaussian\n",
    "     process conditioned on the observations yy at observed locations X_obs.\n",
    "\n",
    "     Inputs:\n",
    "                 X_rest GP test locations\n",
    "                  X_obs locations of observations\n",
    "                     yy observed values\n",
    "                sigma_y observation noise standard deviation\n",
    "                    ell kernel function length scale\n",
    "                sigma_f kernel function standard deviation\n",
    "\n",
    "     Outputs:\n",
    "           rest_cond_mu mean at each location in X_rest\n",
    "          rest_cond_cov covariance matrix between function values at all test locations\n",
    "    \"\"\"\n",
    "    X_rest = X_rest[:, None]\n",
    "    X_obs = X_obs[:, None]\n",
    "    K_rest = gauss_kernel_fn(X_rest, X_rest, ell, sigma_f)\n",
    "    K_rest_obs = gauss_kernel_fn(X_rest, X_obs, ell, sigma_f)\n",
    "    K_obs = gauss_kernel_fn(X_obs, X_obs, ell, sigma_f)\n",
    "    M = K_obs + sigma_y**2 * np.eye(yy.size)\n",
    "    M_cho, M_low = cho_factor(M)\n",
    "    rest_cond_mu = np.dot(K_rest_obs, cho_solve((M_cho, M_low), yy))\n",
    "    rest_cond_cov = K_rest - np.dot(K_rest_obs, cho_solve((M_cho, M_low), K_rest_obs.T))\n",
    "\n",
    "    return rest_cond_mu, rest_cond_cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./ct_data.npz')\n",
    "X_train = data['X_train']; X_val = data['X_val']; X_test = data['X_test']\n",
    "y_train = data['y_train']; y_val = data['y_val']; y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the training positions in y_train is -0.0000 and the standard error of the mean is 0.0050.\n",
      "The mean of the 5,785 positions in the y_val is -0.2160 and the standard error of the mean is  0.0129.\n",
      "The mean of the first 5,785 entries in the y_train is -0.4425 and the standard error of the mean is  0.0119.\n"
     ]
    }
   ],
   "source": [
    "print(f'The mean of the training positions in y_train is {np.mean(y_train):.4f} and the standard error of the mean is {np.std(y_train)/np.sqrt(len(y_train)):.4f}.')\n",
    "print(f'The mean of the 5,785 positions in the y_val is {np.mean(y_val):.4f} and the standard error of the mean is  {np.std(y_val)/np.sqrt(len(y_val)):.4f}.')\n",
    "print(f'The mean of the first 5,785 entries in the y_train is {np.mean(y_train[0:5785,]):.4f} and the standard error of the mean is  {np.std(y_train[0:5785,])/np.sqrt(5785):.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain how your results demonstrate that these standard error bars do not reliably indicate what the average of locations in future CT slice data will be. Why are standard error bars misleading here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ni hao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column indexes of constants: [ 59  69 179 189 351].\n",
      "Column indexes of dupulicates: [ 69  78  79 179 188 189 199 287 351 359].\n",
      "Column indexes for removal: [ 59  69  78  79 179 188 189 199 287 351 359].\n"
     ]
    }
   ],
   "source": [
    "# stage one: constant columns\n",
    "index1 = np.where(np.var(X_train, axis=0) == 0)[0]\n",
    "print(f'Column indexes of constants: {index1}.')\n",
    "\n",
    "# stage two: identical columns\n",
    "index2 = np.delete(np.arange(0, X_train.shape[1]), np.sort(np.unique(X_train, axis=1, return_index=True)[1]))\n",
    "print(f'Column indexes of dupulicates: {index2}.')\n",
    "\n",
    "# indeices for removal\n",
    "index_remove = np.unique(np.concatenate((index1, index2), axis=0))\n",
    "print(f'Column indexes for removal: {index_remove}.')\n",
    "\n",
    "# removal\n",
    "X_train = np.delete(X_train, index_remove, axis=1)\n",
    "X_val = np.delete(X_val, index_remove, axis=1)\n",
    "X_test = np.delete(X_test, index_remove, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report which columns of the X_... arrays you remove at each of the two stages. Report these as 0-based indexes. (For the second stage, you might report indexes in the original array, or after you did the first stage. It doesn’t matter, as long as your code is clear and correct.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linreg(X, yy, alpha):\n",
    "    n, d = X.shape[0], X.shape[1]\n",
    "    y_tilde = np.concatenate((yy, np.zeros(d)))\n",
    "    Phi = np.concatenate((X, np.ones((n, 1))), axis=1)\n",
    "    diag = np.diag(np.append(np.sqrt(alpha) * np.ones((d)), [0]))\n",
    "    Phi_tilde = np.concatenate((Phi, diag))\n",
    "    y_tilde = np.concatenate((yy, np.zeros((d+1))))\n",
    "    w_tilde = np.linalg.lstsq(Phi_tilde, y_tilde, rcond=None)[0]\n",
    "    ww, bb = w_tilde[0:-1], w_tilde[-1]\n",
    "    return ww, bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit X_train and y_train with alpha = 30\n",
    "w_ls, b_ls = fit_linreg(X=X_train, yy=y_train, alpha=30)\n",
    "para_ls = np.concatenate((w_ls, b_ls), axis=None)\n",
    "pred_ls = np.concatenate((X_train, np.ones((X_train.shape[0], 1))), axis=1) @ para_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_linreg_gradopt(X, yy, 30)\n",
    "w_gd, b_gd = fit_linreg_gradopt(X=X_train, yy=y_train, alpha=30)\n",
    "para_gd = np.concatenate((w_ls, b_ls), axis=None)\n",
    "pred_gd = np.concatenate((X_train, np.ones((X_train.shape[0], 1))), axis=1) @ para_gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE using least square method is 0.3568.\n",
      "The RMSE using gradient descent method is 0.3568.\n"
     ]
    }
   ],
   "source": [
    "# Report RMSE = sqrt(||y - f(x;w,b)||^2 / N)\n",
    "RMSE_ls = np.sqrt(np.sum((y_train - pred_ls) ** 2) / len(y_train))\n",
    "RMSE_gd = np.sqrt(np.sum((y_train - pred_gd) ** 2) / len(y_train))\n",
    "print(f'The RMSE using least square method is {RMSE_ls:.4f}.')\n",
    "print(f'The RMSE using gradient descent method is {RMSE_gd:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logreg_gradopt(X, yy, alpha):\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    init = (np.zeros(D), np.array(0))\n",
    "    ww, bb = minimize_list(logreg_cost, init, args)\n",
    "    return ww, bb\n",
    "\n",
    "def logreg_prob(X, ww, bb):\n",
    "    return 1 / (1 + np.exp(- (X @ ww + bb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20 # number of thresholded classification problems to fit\n",
    "mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)\n",
    "prob_train = np.zeros((len(X_train), K))\n",
    "prob_val = np.zeros((len(X_val), K))\n",
    "para_w = np.zeros((X_train.shape[1], K))\n",
    "para_b = np.zeros(K)\n",
    "\n",
    "for kk in range(K):\n",
    "    labels = (y_train > thresholds[kk]).astype(int)\n",
    "    w_logr, b_logr = fit_logreg_gradopt(X=X_train, yy=labels, alpha=30)\n",
    "    para_w[:,kk] = w_logr\n",
    "    para_b[kk] = b_logr\n",
    "    prob_train[:,kk] = logreg_prob(X=X_train, ww=w_logr, bb=b_logr)\n",
    "    prob_val[:,kk] = logreg_prob(X=X_val, ww=w_logr, bb=b_logr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit regularised linear regression based on the results of logistics regression\n",
    "# least square\n",
    "w_ls_log, b_ls_log = fit_linreg(X=prob_train, yy=y_train, alpha=30)\n",
    "para_ls_log = np.concatenate((w_ls_log, b_ls_log), axis=None)\n",
    "pred_ls_log = np.concatenate((prob_train, np.ones((prob_train.shape[0], 1))), axis=1) @ para_ls_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "w_gd_log, b_gd_log = fit_linreg_gradopt(X=prob_train, yy=y_train, alpha=30)\n",
    "para_gd_log = np.concatenate((w_gd_log, b_gd_log), axis=None)\n",
    "pred_gd_log = np.concatenate((prob_train, np.ones((prob_train.shape[0], 1))), axis=1) @ para_gd_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using the TRAIN set!\n",
      "The RMSE using least square method is 0.1544.\n",
      "The RMSE using gradient descent method is 0.1544.\n"
     ]
    }
   ],
   "source": [
    "# Report RMSE = sqrt(||y - f(x;w,b)||^2 / N)\n",
    "# This is for train set\n",
    "RMSE_ls_log_train = np.sqrt(np.sum((y_train - pred_ls_log) ** 2) / len(y_train))\n",
    "RMSE_gd_log_train = np.sqrt(np.sum((y_train - pred_gd_log) ** 2) / y_train.shape[0])\n",
    "\n",
    "print('We are using the TRAIN set!')\n",
    "print(f'The RMSE using least square method is {RMSE_ls_log_train:.4f}.')\n",
    "print(f'The RMSE using gradient descent method is {RMSE_gd_log_train:.4f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using the VAL set!\n",
      "The RMSE using least square method is 0.2542.\n",
      "The RMSE using gradient descent method is 0.2542.\n"
     ]
    }
   ],
   "source": [
    "# Report RMSE = sqrt(||y - f(x;w,b)||^2 / N)\n",
    "# This is for val set\n",
    "pred_ls_log_val = np.concatenate((prob_val, np.ones((prob_val.shape[0], 1))), axis=1) @ para_ls_log\n",
    "pred_gd_log_val = np.concatenate((prob_val, np.ones((prob_val.shape[0], 1))), axis=1) @ para_gd_log\n",
    "\n",
    "RMSE_ls_log_val = np.sqrt(np.sum((y_val - pred_ls_log_val) ** 2) / len(y_val))\n",
    "RMSE_gd_log_val = np.sqrt(np.sum((y_val - pred_gd_log_val) ** 2) / len(y_val))\n",
    "\n",
    "print('We are using the VAL set!')\n",
    "print(f'The RMSE using least square method is {RMSE_ls_log_val:.4f}.')\n",
    "print(f'The RMSE using gradient descent method is {RMSE_gd_log_val:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using the TRAIN set!\n",
      "The RMSE of nn with random initialisaiton is 0.1418.\n"
     ]
    }
   ],
   "source": [
    "# minimise nn cost using minimize_list with random intialisation from lecture note\n",
    "\n",
    "# random initialisation\n",
    "d, k = X_train.shape[1], 20\n",
    "ww = 0.1*np.random.randn(k)/np.sqrt(k)\n",
    "bb = 0.1*np.random.randn()/np.sqrt(k)\n",
    "V = 0.1*np.random.randn(k, d)/np.sqrt(k)\n",
    "bk = 0.1*np.random.randn(k)/np.sqrt(k)\n",
    "nn_init = (ww, bb, V, bk)\n",
    "\n",
    "# X_train, y_train and alpha\n",
    "nn_alpha = 30\n",
    "nn_args = (X_train, y_train, nn_alpha)\n",
    "\n",
    "params_random = minimize_list(nn_cost, nn_init, nn_args)\n",
    "\n",
    "pred_nn_random_train = nn_cost(params_random, X=X_train)\n",
    "RMSE_nn_random_train = np.sqrt(np.sum((y_train - pred_nn_random_train) ** 2) / len(y_train))\n",
    "\n",
    "print('We are using the TRAIN set!')\n",
    "print(f'The RMSE of nn with random initialisaiton is {RMSE_nn_random_train:.4f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using the TRAIN set!\n",
      "The RMSE of nn with Q3 initialisaiton is 0.1396.\n"
     ]
    }
   ],
   "source": [
    "# minimise nn cost using minimize_list with Q3 intialisation\n",
    "\n",
    "# Q3 initialisation\n",
    "nn_init_q3 = (w_ls_log, b_ls_log, para_w.T, para_b)\n",
    "\n",
    "# X_train, y_train and alpha\n",
    "nn_alpha = 30\n",
    "nn_args = (X_train, y_train, nn_alpha)\n",
    "\n",
    "params_q3 = minimize_list(nn_cost, nn_init_q3, nn_args)\n",
    "\n",
    "pred_nn_q3_train = nn_cost(params_q3, X=X_train)\n",
    "RMSE_nn_q3_train = np.sqrt(np.sum((y_train - pred_nn_q3_train) ** 2) / len(y_train))\n",
    "\n",
    "print('We are using the TRAIN set!')\n",
    "print(f'The RMSE of nn with Q3 initialisaiton is {RMSE_nn_q3_train:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayesian optimisation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
