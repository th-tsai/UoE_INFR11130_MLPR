{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import cho_factor, cho_solve\n",
    "\n",
    "def params_unwrap(param_vec, shapes, sizes):\n",
    "    \"\"\"Helper routine for minimize_list\"\"\"\n",
    "    args = []\n",
    "    pos = 0\n",
    "    for i in range(len(shapes)):\n",
    "        sz = sizes[i]\n",
    "        args.append(param_vec[pos:pos+sz].reshape(shapes[i]))\n",
    "        pos += sz\n",
    "    return args\n",
    "\n",
    "\n",
    "def params_wrap(param_list):\n",
    "    \"\"\"Helper routine for minimize_list\"\"\"\n",
    "    param_list = [np.array(x) for x in param_list]\n",
    "    shapes = [x.shape for x in param_list]\n",
    "    sizes = [x.size for x in param_list]\n",
    "    param_vec = np.zeros(sum(sizes))\n",
    "    pos = 0\n",
    "    for param in param_list:\n",
    "        sz = param.size\n",
    "        param_vec[pos:pos+sz] = param.ravel()\n",
    "        pos += sz\n",
    "    unwrap = lambda pvec: params_unwrap(pvec, shapes, sizes)\n",
    "    return param_vec, unwrap\n",
    "\n",
    "\n",
    "def minimize_list(cost, init_list, args):\n",
    "    \"\"\"Optimize a list of arrays (wrapper of scipy.optimize.minimize)\n",
    "\n",
    "    The input function \"cost\" should take a list of parameters,\n",
    "    followed by any extra arguments:\n",
    "        cost(init_list, *args)\n",
    "    should return the cost of the initial condition, and a list in the same\n",
    "    format as init_list giving gradients of the cost wrt the parameters.\n",
    "\n",
    "    The options to the optimizer have been hard-coded. You may wish\n",
    "    to change disp to True to get more diagnostics. You may want to\n",
    "    decrease maxiter while debugging. Although please report all results\n",
    "    in Q2-5 using maxiter=500.\n",
    "    \"\"\"\n",
    "    opt = {'maxiter': 500, 'disp': False}\n",
    "    init, unwrap = params_wrap(init_list)\n",
    "    def wrap_cost(vec, *args):\n",
    "        E, params_bar = cost(unwrap(vec), *args)\n",
    "        vec_bar, _ = params_wrap(params_bar)\n",
    "        return E, vec_bar\n",
    "    res = minimize(wrap_cost, init, args, 'L-BFGS-B', jac=True, options=opt)\n",
    "    return unwrap(res.x)\n",
    "\n",
    "\n",
    "def linreg_cost(params, X, yy, alpha):\n",
    "    \"\"\"Regularized least squares cost function and gradients\n",
    "\n",
    "    Can be optimized with minimize_list -- see fit_linreg_gradopt for a\n",
    "    demonstration.\n",
    "\n",
    "    Inputs:\n",
    "    params: tuple (ww, bb): weights ww (D,), bias bb scalar\n",
    "         X: N,D design matrix of input features\n",
    "        yy: N,  real-valued targets\n",
    "     alpha: regularization constant\n",
    "\n",
    "    Outputs: (E, [ww_bar, bb_bar]), cost and gradients\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb = params\n",
    "\n",
    "    # forward computation of error\n",
    "    ff = np.dot(X, ww) + bb\n",
    "    res = ff - yy\n",
    "    E = np.dot(res, res) + alpha*np.dot(ww, ww)\n",
    "\n",
    "    # reverse computation of gradients\n",
    "    ff_bar = 2*res\n",
    "    bb_bar = np.sum(ff_bar)\n",
    "    ww_bar = np.dot(X.T, ff_bar) + 2*alpha*ww\n",
    "\n",
    "    return E, [ww_bar, bb_bar]\n",
    "\n",
    "\n",
    "def fit_linreg_gradopt(X, yy, alpha):\n",
    "    \"\"\"\n",
    "    fit a regularized linear regression model with gradient opt\n",
    "\n",
    "         ww, bb = fit_linreg_gradopt(X, yy, alpha)\n",
    "\n",
    "     Find weights and bias by using a gradient-based optimizer\n",
    "     (minimize_list) to improve the regularized least squares cost:\n",
    "\n",
    "       np.sum(((np.dot(X,ww) + bb) - yy)**2) + alpha*np.dot(ww,ww)\n",
    "\n",
    "     Inputs:\n",
    "             X N,D design matrix of input features\n",
    "            yy N,  real-valued targets\n",
    "         alpha     scalar regularization constant\n",
    "\n",
    "     Outputs:\n",
    "            ww D,  fitted weights\n",
    "            bb     scalar fitted bias\n",
    "    \"\"\"\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    init = (np.zeros(D), np.array(0))\n",
    "    ww, bb = minimize_list(linreg_cost, init, args)\n",
    "    return ww, bb\n",
    "\n",
    "\n",
    "def logreg_cost(params, X, yy, alpha):\n",
    "    \"\"\"Regularized logistic regression cost function and gradients\n",
    "\n",
    "    Can be optimized with minimize_list -- see fit_linreg_gradopt for a\n",
    "    demonstration of fitting a similar function.\n",
    "\n",
    "    Inputs:\n",
    "    params: tuple (ww, bb): weights ww (D,), bias bb scalar\n",
    "         X: N,D design matrix of input features\n",
    "        yy: N,  real-valued targets\n",
    "     alpha: regularization constant\n",
    "\n",
    "    Outputs: (E, [ww_bar, bb_bar]), cost and gradients\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb = params\n",
    "\n",
    "    # Force targets to be +/- 1\n",
    "    yy = 2*(yy==1) - 1\n",
    "\n",
    "    # forward computation of error\n",
    "    aa = yy*(np.dot(X, ww) + bb)\n",
    "    sigma = 1/(1 + np.exp(-aa))\n",
    "    E = -np.sum(np.log(sigma)) + alpha*np.dot(ww, ww)\n",
    "\n",
    "    # reverse computation of gradients\n",
    "    aa_bar = sigma - 1\n",
    "    bb_bar = np.dot(aa_bar, yy)\n",
    "    ww_bar = np.dot(X.T, yy*aa_bar) + 2*alpha*ww\n",
    "\n",
    "    return E, (ww_bar, bb_bar)\n",
    "\n",
    "\n",
    "def nn_cost(params, X, yy=None, alpha=None):\n",
    "    \"\"\"NN_COST simple neural network cost function and gradients, or predictions\n",
    "\n",
    "           E, params_bar = nn_cost([ww, bb, V, bk], X, yy, alpha)\n",
    "                    pred = nn_cost([ww, bb, V, bk], X)\n",
    "\n",
    "     Cost function E can be minimized with minimize_list\n",
    "\n",
    "     Inputs:\n",
    "             params (ww, bb, V, bk), where:\n",
    "                    --------------------------------\n",
    "                        ww K,  hidden-output weights\n",
    "                        bb     scalar output bias\n",
    "                        V  K,D hidden-input weights\n",
    "                        bk K,  hidden biases\n",
    "                    --------------------------------\n",
    "                  X N,D input design matrix\n",
    "                 yy N,  regression targets\n",
    "              alpha     scalar regularization for weights\n",
    "\n",
    "     Outputs:\n",
    "                     E  sum of squares error\n",
    "            params_bar  gradients wrt params, same format as params\n",
    "     OR\n",
    "               pred N,  predictions if only params and X are given as inputs\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb, V, bk = params\n",
    "\n",
    "    # Forwards computation of cost\n",
    "    A = np.dot(X, V.T) + bk[None,:] # N,K\n",
    "    P = 1 / (1 + np.exp(-A)) # N,K\n",
    "    F = np.dot(P, ww) + bb # N,\n",
    "    if yy is None:\n",
    "        # user wants prediction rather than training signal:\n",
    "        return F\n",
    "    res = F - yy # N,\n",
    "    E = np.dot(res, res) + alpha*(np.sum(V*V) + np.dot(ww,ww)) # 1x1\n",
    "\n",
    "    # Reverse computation of gradients\n",
    "    F_bar = 2*res # N,\n",
    "    ww_bar = np.dot(P.T, F_bar) + 2*alpha*ww # K,\n",
    "    bb_bar = np.sum(F_bar) # scalar\n",
    "    P_bar = np.dot(F_bar[:,None], ww[None,:]) # N,K\n",
    "    A_bar = P_bar * P * (1 - P) # N,K\n",
    "    V_bar = np.dot(A_bar.T, X) + 2*alpha*V # K,D\n",
    "    bk_bar = np.sum(A_bar, 0)\n",
    "\n",
    "    return E, (ww_bar, bb_bar, V_bar, bk_bar)\n",
    "\n",
    "\n",
    "def rbf_fn(X1, X2):\n",
    "    \"\"\"Helper routine for gp_post_par\"\"\"\n",
    "    return np.exp((np.dot(X1,(2*X2.T))-np.sum(X1*X1,1)[:,None]) - np.sum(X2*X2,1)[None,:])\n",
    "\n",
    "\n",
    "def gauss_kernel_fn(X1, X2, ell, sigma_f):\n",
    "    \"\"\"Helper routine for gp_post_par\"\"\"\n",
    "    return sigma_f**2 * rbf_fn(X1/(np.sqrt(2)*ell), X2/(np.sqrt(2)*ell))\n",
    "\n",
    "\n",
    "def gp_post_par(X_rest, X_obs, yy, sigma_y=0.05, ell=5.0, sigma_f=0.1):\n",
    "    \"\"\"GP_POST_PAR means and covariances of a posterior Gaussian process\n",
    "\n",
    "         rest_cond_mu, rest_cond_cov = gp_post_par(X_rest, X_obs, yy)\n",
    "         rest_cond_mu, rest_cond_cov = gp_post_par(X_rest, X_obs, yy, sigma_y, ell, sigma_f)\n",
    "\n",
    "     Calculate the means and covariances at all test locations of the posterior Gaussian\n",
    "     process conditioned on the observations yy at observed locations X_obs.\n",
    "\n",
    "     Inputs:\n",
    "                 X_rest GP test locations\n",
    "                  X_obs locations of observations\n",
    "                     yy observed values\n",
    "                sigma_y observation noise standard deviation\n",
    "                    ell kernel function length scale\n",
    "                sigma_f kernel function standard deviation\n",
    "\n",
    "     Outputs:\n",
    "           rest_cond_mu mean at each location in X_rest\n",
    "          rest_cond_cov covariance matrix between function values at all test locations\n",
    "    \"\"\"\n",
    "    X_rest = X_rest[:, None]\n",
    "    X_obs = X_obs[:, None]\n",
    "    K_rest = gauss_kernel_fn(X_rest, X_rest, ell, sigma_f)\n",
    "    K_rest_obs = gauss_kernel_fn(X_rest, X_obs, ell, sigma_f)\n",
    "    K_obs = gauss_kernel_fn(X_obs, X_obs, ell, sigma_f)\n",
    "    M = K_obs + sigma_y**2 * np.eye(yy.size)\n",
    "    M_cho, M_low = cho_factor(M)\n",
    "    rest_cond_mu = np.dot(K_rest_obs, cho_solve((M_cho, M_low), yy))\n",
    "    rest_cond_cov = K_rest - np.dot(K_rest_obs, cho_solve((M_cho, M_low), K_rest_obs.T))\n",
    "\n",
    "    return rest_cond_mu, rest_cond_cov\n",
    "\n",
    "data = np.load('./ct_data.npz')\n",
    "X_train = data['X_train']; X_val = data['X_val']; X_test = data['X_test']\n",
    "y_train = data['y_train']; y_val = data['y_val']; y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1 a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the training positions in y_train is -0.0000 and the standard error of the mean is 0.0050.\n",
      "The mean of the 5,785 positions in the y_val is -0.2160 and the standard error of the mean is 0.0129.\n",
      "The mean of the first 5,785 entries in the y_train is -0.4425 and the standard error of the mean is 0.0119.\n"
     ]
    }
   ],
   "source": [
    "print(f'The mean of the training positions in y_train is {np.mean(y_train):.4f} and the standard error of the mean is {np.std(y_train)/np.sqrt(len(y_train)):.4f}.')\n",
    "print(f'The mean of the 5,785 positions in the y_val is {np.mean(y_val):.4f} and the standard error of the mean is {np.std(y_val)/np.sqrt(len(y_val)):.4f}.')\n",
    "print(f'The mean of the first 5,785 entries in the y_train is {np.mean(y_train[0:5785,]):.4f} and the standard error of the mean is {np.std(y_train[0:5785,])/np.sqrt(5785):.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1 b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column indexes of constants: [ 59  69 179 189 351].\n",
      "Column indexes of dupulicates: [ 69  78  79 179 188 189 199 287 351 359].\n",
      "Column indexes for removal: [ 59  69  78  79 179 188 189 199 287 351 359].\n"
     ]
    }
   ],
   "source": [
    "# stage one: constant columns\n",
    "index1 = np.where(np.var(X_train, axis=0) == 0)[0]\n",
    "print(f'Column indexes of constants: {index1}.')\n",
    "\n",
    "# stage two: identical columns\n",
    "index2 = np.delete(np.arange(0, X_train.shape[1]), np.sort(np.unique(X_train, axis=1, return_index=True)[1]))\n",
    "print(f'Column indexes of dupulicates: {index2}.')\n",
    "\n",
    "# indeices for removal\n",
    "index_remove = np.unique(np.concatenate((index1, index2), axis=0))\n",
    "print(f'Column indexes for removal: {index_remove}.')\n",
    "\n",
    "# removal\n",
    "X_train = np.delete(X_train, index_remove, axis=1)\n",
    "X_val = np.delete(X_val, index_remove, axis=1)\n",
    "X_test = np.delete(X_test, index_remove, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE using least square method on the TRAIN set is 0.3568.\n",
      "The RMSE using gradient descent method on the TRAIN set is 0.3568.\n",
      "The RMSE using least square method on the VAL set is 0.4231.\n",
      "The RMSE using gradient descent method on the VAL set is 0.4231.\n"
     ]
    }
   ],
   "source": [
    "# fit_linreg\n",
    "def fit_linreg(X, yy, alpha):\n",
    "    n, d = X.shape[0], X.shape[1]\n",
    "    y_tilde = np.concatenate((yy, np.zeros(d)))\n",
    "    Phi = np.concatenate((X, np.ones((n, 1))), axis=1)\n",
    "    diag = np.diag(np.append(np.sqrt(alpha) * np.ones((d)), [0]))\n",
    "    Phi_tilde = np.concatenate((Phi, diag))\n",
    "    y_tilde = np.concatenate((yy, np.zeros((d+1))))\n",
    "    w_tilde = np.linalg.lstsq(Phi_tilde, y_tilde, rcond=None)[0]\n",
    "    ww, bb = w_tilde[0:-1], w_tilde[-1]\n",
    "    return ww, bb\n",
    "\n",
    "# fit X_train and y_train with alpha = 30 using least square method\n",
    "w_ls, b_ls = fit_linreg(X=X_train, yy=y_train, alpha=30)\n",
    "pred_ls_train = np.concatenate((X_train, np.ones((len(X_train), 1))), axis=1) @ np.concatenate((w_ls, b_ls), axis=None)\n",
    "pred_ls_val = np.concatenate((X_val, np.ones((len(X_val), 1))), axis=1) @ np.concatenate((w_ls, b_ls), axis=None)\n",
    "\n",
    "# fit X_train and y_train with alpha = 30 using gradient descent method\n",
    "w_gd, b_gd = fit_linreg_gradopt(X=X_train, yy=y_train, alpha=30)\n",
    "pred_gd_train = np.concatenate((X_train, np.ones((len(X_train), 1))), axis=1) @ np.concatenate((w_gd, b_gd), axis=None)\n",
    "pred_gd_val = np.concatenate((X_val, np.ones((len(X_val), 1))), axis=1) @ np.concatenate((w_gd, b_gd), axis=None)\n",
    "\n",
    "# RMSE\n",
    "RMSE_ls_train = np.sqrt(np.mean((y_train - pred_ls_train) ** 2))\n",
    "RMSE_gd_train = np.sqrt(np.mean((y_train - pred_gd_train) ** 2))\n",
    "\n",
    "print(f'The RMSE using least square method on the TRAIN set is {RMSE_ls_train:.4f}.')\n",
    "print(f'The RMSE using gradient descent method on the TRAIN set is {RMSE_gd_train:.4f}.')\n",
    "\n",
    "RMSE_ls_val = np.sqrt(np.mean((y_val - pred_ls_val) ** 2))\n",
    "RMSE_gd_val = np.sqrt(np.mean((y_val - pred_gd_val) ** 2))\n",
    "\n",
    "print(f'The RMSE using least square method on the VAL set is {RMSE_ls_val:.4f}.')\n",
    "print(f'The RMSE using gradient descent method on the VAL set is {RMSE_gd_val:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE of the TRAIN set is 0.1544.\n",
      "The RMSE of the VAL set is 0.2542.\n"
     ]
    }
   ],
   "source": [
    "def fit_logreg_gradopt(X, yy, alpha):\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    init = (np.zeros(D), np.array([0]))\n",
    "    ww, bb = minimize_list(logreg_cost, init, args)\n",
    "    return ww, bb\n",
    "\n",
    "def logreg_prob(X, ww, bb):\n",
    "    return 1 / (1 + np.exp(- (X @ ww + bb)))\n",
    "\n",
    "K = 20 # number of thresholded classification problems to fit\n",
    "mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)\n",
    "prob_train = np.zeros((len(X_train), K))\n",
    "prob_val = np.zeros((len(X_val), K))\n",
    "w_logr = np.zeros((X_train.shape[1], K))\n",
    "b_logr = np.zeros(K)\n",
    "\n",
    "for kk in range(K):\n",
    "    labels = (y_train > thresholds[kk]).astype(int)\n",
    "    w_logr_kk, b_logr_kk = fit_logreg_gradopt(X=X_train, yy=labels, alpha=30)\n",
    "    w_logr[:,kk] = w_logr_kk\n",
    "    b_logr[kk] = b_logr_kk[0]\n",
    "    prob_train[:,kk] = logreg_prob(X=X_train, ww=w_logr[:,kk], bb=b_logr[kk])\n",
    "    prob_val[:,kk] = logreg_prob(X=X_val, ww=w_logr[:,kk], bb=b_logr[kk])\n",
    "\n",
    "\n",
    "w_logr_lr, b_logr_lr = fit_linreg_gradopt(X=prob_train, yy=y_train, alpha=30)\n",
    "param_logr_lr = np.concatenate((w_logr_lr, b_logr_lr), axis=None)\n",
    "\n",
    "pred_logr_lr_train = np.concatenate((prob_train, np.ones((len(prob_train), 1))), axis=1) @ param_logr_lr\n",
    "RMSE_log_lr_train = np.sqrt(np.mean((y_train - pred_logr_lr_train) ** 2))\n",
    "print(f'The RMSE of the TRAIN set is {RMSE_log_lr_train:.4f}.')\n",
    "\n",
    "pred_log_lr_val = np.concatenate((prob_val, np.ones((len(prob_val), 1))), axis=1) @ param_logr_lr\n",
    "RMSE_log_lr_val = np.sqrt(np.mean((y_val - pred_log_lr_val) ** 2))\n",
    "print(f'The RMSE of the VAL set is {RMSE_log_lr_val:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE of nn with random initialisaiton using the TRAIN set is 0.1475.\n",
      "The RMSE of nn with random initialisaiton using the VAL set is 0.2749.\n",
      "The RMSE of nn with Q3 initialisaiton using the TRAIN set is 0.1394.\n",
      "The RMSE of nn with q3 initialisaiton using the VAL set is 0.2703.\n"
     ]
    }
   ],
   "source": [
    "# random initialisation\n",
    "d, k = X_train.shape[1], 20\n",
    "ww = 0.1*np.random.randn(k)/np.sqrt(k)\n",
    "bb = 0.1*np.random.randn()/np.sqrt(k)\n",
    "V = 0.1*np.random.randn(k, d)/np.sqrt(k)\n",
    "bk = 0.1*np.random.randn(k)/np.sqrt(k)\n",
    "nn_init_random = (ww, bb, V, bk)\n",
    "\n",
    "# X_train, y_train and alpha\n",
    "nn_alpha = 30\n",
    "nn_args = (X_train, y_train, nn_alpha)\n",
    "\n",
    "nn_param_random = minimize_list(nn_cost, nn_init_random, nn_args)\n",
    "\n",
    "pred_nn_random_train = nn_cost(nn_param_random, X=X_train)\n",
    "RMSE_nn_random_train = np.sqrt(np.mean((y_train - pred_nn_random_train) ** 2))\n",
    "\n",
    "print(f'The RMSE of nn with random initialisaiton using the TRAIN set is {RMSE_nn_random_train:.4f}.')\n",
    "\n",
    "pred_nn_random_val = nn_cost(nn_param_random, X=X_val)\n",
    "RMSE_nn_random_val = np.sqrt(np.mean((y_val - pred_nn_random_val) ** 2))\n",
    "\n",
    "print(f'The RMSE of nn with random initialisaiton using the VAL set is {RMSE_nn_random_val:.4f}.')\n",
    "\n",
    "# Q3 initialisation\n",
    "nn_init_q3 = (w_logr_lr, b_logr_lr, w_logr.T, b_logr)\n",
    "\n",
    "# X_train, y_train and alpha\n",
    "nn_alpha = 30\n",
    "nn_args = (X_train, y_train, nn_alpha)\n",
    "\n",
    "nn_params_q3 = minimize_list(nn_cost, nn_init_q3, nn_args)\n",
    "\n",
    "pred_nn_q3_train = nn_cost(nn_params_q3, X=X_train)\n",
    "RMSE_nn_q3_train = np.sqrt(np.mean((y_train - pred_nn_q3_train) ** 2))\n",
    "\n",
    "print(f'The RMSE of nn with Q3 initialisaiton using the TRAIN set is {RMSE_nn_q3_train:.4f}.')\n",
    "\n",
    "pred_nn_q3_val = nn_cost(nn_params_q3, X=X_val)\n",
    "RMSE_nn_q3_val = np.sqrt(np.mean((y_val - pred_nn_q3_val) ** 2))\n",
    "\n",
    "print(f'The RMSE of nn with q3 initialisaiton using the VAL set is {RMSE_nn_q3_val:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE_val is 0.2707 while alpha is 30.00.\n",
      "The above RMSE is a baseline. \n",
      "\n",
      "Pick three alphas\n",
      "\n",
      "The RMSE_val is 0.2693 while alpha is 21.12.\n",
      "The RMSE_val is 0.2797 while alpha is 47.84.\n",
      "The RMSE_val is 0.2675 while alpha is 20.44.\n",
      "\n",
      "---------- Iteration 1 ----------\n",
      "The maximal PI is 0.4706 while alpha equals to 16.88.\n",
      "The RMSE_val is 0.2692 while alpha is 16.88.\n",
      "\n",
      "---------- Iteration 2 ----------\n",
      "The maximal PI is 0.4592 while alpha equals to 25.38.\n",
      "The RMSE_val is 0.2696 while alpha is 25.38.\n",
      "\n",
      "---------- Iteration 3 ----------\n",
      "The maximal PI is 0.4529 while alpha equals to 0.00.\n",
      "The RMSE_val is 0.2723 while alpha is 0.00.\n",
      "\n",
      "---------- Iteration 4 ----------\n",
      "The maximal PI is 0.4500 while alpha equals to 33.28.\n",
      "The RMSE_val is 0.2726 while alpha is 33.28.\n",
      "\n",
      "---------- Iteration 5 ----------\n",
      "The maximal PI is 0.4493 while alpha equals to 10.82.\n",
      "The RMSE_val is 0.2666 while alpha is 10.82.\n",
      "\n",
      "The best RMSE of the VAL set is 0.2666 while alpha is 10.82.\n",
      "The alpha is 10.82.\n",
      "The RMSE of nn with random initialisaiton using the TRAIN set is 0.1003.\n",
      "The RMSE of nn with random initialisaiton using the VAL set is 0.2637.\n",
      "The RMSE of nn with random initialisaiton using the TEST set is 0.2937.\n"
     ]
    }
   ],
   "source": [
    "def train_nn_reg(XX, XX_val, yy, yy_val, params, alpha, report_train=False, report_val=True):\n",
    "    args = (XX, yy, alpha)\n",
    "    params_bar = minimize_list(nn_cost, params, args)\n",
    "    pred_train = nn_cost(params_bar, X=XX)\n",
    "    RMSE_train = np.sqrt(np.mean((yy - pred_train) ** 2))\n",
    "    pred_val = nn_cost(params_bar, X=XX_val)\n",
    "    RMSE_val = np.sqrt(np.mean((yy_val - pred_val) ** 2))\n",
    "    if report_train:\n",
    "        print(f'The RMSE_train is {RMSE_train:.4f} while alpha is {alpha:.2f}.')\n",
    "    if report_val:\n",
    "        print(f'The RMSE_val is {RMSE_val:.4f} while alpha is {alpha:.2f}.')\n",
    "    return RMSE_val\n",
    "\n",
    "# alpha set\n",
    "alpha_set = np.arange(0, 50, 0.02)\n",
    "\n",
    "# initialisation\n",
    "d, k = X_train.shape[1], 20\n",
    "ww = 0.1*np.random.randn(k)/np.sqrt(k)\n",
    "bb = 0.1*np.random.randn()/np.sqrt(k)\n",
    "V = 0.1*np.random.randn(k, d)/np.sqrt(k)\n",
    "bk = 0.1*np.random.randn(k)/np.sqrt(k)\n",
    "nn_init_random = (ww, bb, V, bk)\n",
    "points_num = 3\n",
    "alpha_init = 30\n",
    "\n",
    "RMSE_found = np.array([])\n",
    "alpha_found = np.array([])\n",
    "\n",
    "# baseline RMSE_val\n",
    "RMSE_alpha_base = train_nn_reg(XX=X_train, XX_val=X_val, yy=y_train, yy_val=y_val, \n",
    "                               params=nn_init_random, alpha=alpha_init)\n",
    "print('The above RMSE is a baseline. \\n')\n",
    "\n",
    "# pick three observed alphas\n",
    "pick = np.random.randint(0, len(alpha_set), points_num)\n",
    "alpha_train = alpha_set[pick]\n",
    "alpha_acquisition = np.delete(alpha_set, pick)\n",
    "alpha_set = np.delete(alpha_set, pick)\n",
    "\n",
    "print('Pick three alphas\\n')\n",
    "for i in range(points_num):\n",
    "    RMSE_found = np.append(RMSE_found, train_nn_reg(XX=X_train, XX_val=X_val, yy=y_train, yy_val=y_val, \n",
    "                                                    params=nn_init_random, alpha=alpha_train[i]))\n",
    "    alpha_found = np.append(alpha_found, alpha_train[i])\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    y = np.log(RMSE_alpha_base) - np.log(RMSE_found)\n",
    "    mu, cov = gp_post_par(X_rest=alpha_acquisition, X_obs=alpha_found, yy=y)\n",
    "    pi = scipy.stats.norm.cdf((mu - y.max()) / np.sqrt(np.diag(cov)))\n",
    "    alpha_opt = alpha_acquisition[np.argmax(pi)]\n",
    "    print(f'\\n---------- Iteration {i+1} ----------')\n",
    "    print(f'The maximal PI is {pi.max():.4f} while alpha equals to {alpha_opt:.2f}.')\n",
    "\n",
    "    alpha_set = np.delete(alpha_set, np.where(alpha_set == alpha_opt))\n",
    "    RMSE_found = np.append(RMSE_found, train_nn_reg(XX=X_train, XX_val=X_val, yy=y_train, yy_val=y_val, \n",
    "                                                    params=nn_init_random, alpha=alpha_opt))\n",
    "    alpha_found = np.append(alpha_found, alpha_opt)\n",
    "    alpha_acquisition = np.delete(alpha_acquisition, alpha_acquisition == alpha_opt)\n",
    "\n",
    "best_RMSE_val = RMSE_found.min()\n",
    "best_alpha = alpha_found[RMSE_found == RMSE_found.min()][0]\n",
    "print(f'\\nThe best RMSE of the VAL set is {best_RMSE_val:.4f} while alpha is {best_alpha:.2f}.')\n",
    "\n",
    "# random initialisation\n",
    "d, k = X_train.shape[1], 20\n",
    "ww = 0.1*np.random.randn(k)/np.sqrt(k)\n",
    "bb = 0.1*np.random.randn()/np.sqrt(k)\n",
    "V = 0.1*np.random.randn(k, d)/np.sqrt(k)\n",
    "bk = 0.1*np.random.randn(k)/np.sqrt(k)\n",
    "nn_init_random = (ww, bb, V, bk)\n",
    "\n",
    "# X_train, y_train and alpha\n",
    "nn_alpha = best_alpha\n",
    "nn_args = (X_train, y_train, nn_alpha)\n",
    "\n",
    "nn_param_random = minimize_list(nn_cost, nn_init_random, nn_args)\n",
    "\n",
    "pred_nn_random_train = nn_cost(nn_param_random, X=X_train)\n",
    "RMSE_nn_random_train = np.sqrt(np.mean((y_train - pred_nn_random_train) ** 2))\n",
    "\n",
    "pred_nn_random_val = nn_cost(nn_param_random, X=X_val)\n",
    "RMSE_nn_random_val = np.sqrt(np.mean((y_val - pred_nn_random_val) ** 2))\n",
    "\n",
    "pred_nn_random_test = nn_cost(nn_param_random, X=X_test)\n",
    "RMSE_nn_random_test = np.sqrt(np.mean((y_test - pred_nn_random_test) ** 2))\n",
    "\n",
    "print(f'The alpha is {nn_alpha}.')\n",
    "print(f'The RMSE of nn with random initialisaiton using the TRAIN set is {RMSE_nn_random_train:.4f}.')\n",
    "print(f'The RMSE of nn with random initialisaiton using the VAL set is {RMSE_nn_random_val:.4f}.')\n",
    "print(f'The RMSE of nn with random initialisaiton using the TEST set is {RMSE_nn_random_test:.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new data dimension is 72.\n",
      "The RMSE_val is 0.2171 while alpha is 30.00.\n",
      "The above RMSE is a baseline. \n",
      "\n",
      "Pick three alphas\n",
      "\n",
      "The RMSE_val is 0.2152 while alpha is 41.64.\n",
      "The RMSE_val is 0.2187 while alpha is 48.78.\n",
      "The RMSE_val is 0.2181 while alpha is 46.46.\n",
      "\n",
      "---------- Iteration 1 ----------\n",
      "The maximal PI is 0.4906 while alpha equals to 38.90.\n",
      "The RMSE_val is 0.2149 while alpha is 38.90.\n",
      "\n",
      "---------- Iteration 2 ----------\n",
      "The maximal PI is 0.4893 while alpha equals to 38.46.\n",
      "The RMSE_val is 0.2167 while alpha is 38.46.\n",
      "\n",
      "---------- Iteration 3 ----------\n",
      "The maximal PI is 0.4607 while alpha equals to 0.00.\n",
      "The RMSE_val is 0.2355 while alpha is 0.00.\n",
      "\n",
      "---------- Iteration 4 ----------\n",
      "The maximal PI is 0.4606 while alpha equals to 21.20.\n",
      "The RMSE_val is 0.2372 while alpha is 21.20.\n",
      "\n",
      "---------- Iteration 5 ----------\n",
      "The maximal PI is 0.4547 while alpha equals to 40.60.\n",
      "The RMSE_val is 0.2163 while alpha is 40.60.\n",
      "\n",
      "The best RMSE of the VAL set is 0.2149 while alpha is 38.90.\n",
      "The alpha is 38.9.\n",
      "The RMSE of nn with random initialisaiton using the TRAIN_pca set is 0.1077.\n",
      "The RMSE of nn with random initialisaiton using the VAL_pca set is 0.2319.\n",
      "The RMSE of nn with random initialisaiton using the TEST_pca set is 0.2470.\n"
     ]
    }
   ],
   "source": [
    "# remove column 271\n",
    "X_train = np.delete(X_train, 271, axis=1)\n",
    "X_val = np.delete(X_val, 271, axis=1)\n",
    "X_test = np.delete(X_test, 271, axis=1)\n",
    "\n",
    "# standardisation\n",
    "X_train_std = (X_train - X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "X_val_std = (X_val - X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "X_test_std = (X_test - X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "\n",
    "# eigenvalues and eigenvectors\n",
    "X_train_std_cov = np.cov(X_train_std, ddof=1, rowvar=False)\n",
    "X_train_std_cov_eigval, X_train_std_cov_eigvec = np.linalg.eig(X_train_std_cov)\n",
    "PC_order = np.argsort(X_train_std_cov_eigval)[::-1]\n",
    "X_train_std_cov_eigval = X_train_std_cov_eigval[PC_order]\n",
    "X_train_std_cov_eigvec = X_train_std_cov_eigvec[:,PC_order]\n",
    "explained_variance = X_train_std_cov_eigval / np.sum(X_train_std_cov_eigval)\n",
    "accum_explained_variance = np.add.accumulate(explained_variance)\n",
    "\n",
    "# retain 80% of information or try others\n",
    "pc = sum(accum_explained_variance <= 0.8) + 1\n",
    "print(f'The new data dimension is {pc}.')\n",
    "\n",
    "# PCA\n",
    "X_train_pca = X_train_std @ X_train_std_cov_eigvec[:,:pc]\n",
    "X_val_pca = X_val_std @ X_train_std_cov_eigvec[:,:pc]\n",
    "X_test_pca = X_test_std @ X_train_std_cov_eigvec[:,:pc]\n",
    "\n",
    "# alpha set\n",
    "alpha_set = np.arange(0, 50, 0.02)\n",
    "\n",
    "# initialisation\n",
    "d, k = X_train_pca.shape[1], 20\n",
    "ww = 0.1*np.random.randn(k)/np.sqrt(k)\n",
    "bb = 0.1*np.random.randn()/np.sqrt(k)\n",
    "V = 0.1*np.random.randn(k, d)/np.sqrt(k)\n",
    "bk = 0.1*np.random.randn(k)/np.sqrt(k)\n",
    "nn_init_random = (ww, bb, V, bk)\n",
    "points_num = 3\n",
    "alpha_init = 30\n",
    "\n",
    "RMSE_found = np.array([])\n",
    "alpha_found = np.array([])\n",
    "\n",
    "# baseline RMSE_val\n",
    "RMSE_alpha_base = train_nn_reg(XX=X_train_pca, XX_val=X_val_pca, yy=y_train, yy_val=y_val, \n",
    "                               params=nn_init_random, alpha=alpha_init)\n",
    "print('The above RMSE is a baseline. \\n')\n",
    "\n",
    "# pick three observed alphas\n",
    "pick = np.random.randint(0, len(alpha_set), points_num)\n",
    "alpha_train = alpha_set[pick]\n",
    "alpha_acquisition = np.delete(alpha_set, pick)\n",
    "alpha_set = np.delete(alpha_set, pick)\n",
    "\n",
    "print('Pick three alphas\\n')\n",
    "for i in range(points_num):\n",
    "    RMSE_found = np.append(RMSE_found, train_nn_reg(XX=X_train_pca, XX_val=X_val_pca, yy=y_train, yy_val=y_val, \n",
    "                                                    params=nn_init_random, alpha=alpha_train[i]))\n",
    "    alpha_found = np.append(alpha_found, alpha_train[i])\n",
    "\n",
    "for i in range(5):\n",
    "    y = np.log(RMSE_alpha_base) - np.log(RMSE_found)\n",
    "    mu, cov = gp_post_par(X_rest=alpha_acquisition, X_obs=alpha_found, yy=y)\n",
    "    pi = scipy.stats.norm.cdf((mu - y.max()) / np.sqrt(np.diag(cov)))\n",
    "    alpha_opt = alpha_acquisition[np.argmax(pi)]\n",
    "    print(f'\\n---------- Iteration {i+1} ----------')\n",
    "    print(f'The maximal PI is {pi.max():.4f} while alpha equals to {alpha_opt:.2f}.')\n",
    "\n",
    "    alpha_set = np.delete(alpha_set, np.where(alpha_set == alpha_opt))\n",
    "    RMSE_found = np.append(RMSE_found, train_nn_reg(XX=X_train_pca, XX_val=X_val_pca, yy=y_train, yy_val=y_val, \n",
    "                                                    params=nn_init_random, alpha=alpha_opt))\n",
    "    alpha_found = np.append(alpha_found, alpha_opt)\n",
    "    alpha_acquisition = np.delete(alpha_acquisition, alpha_acquisition == alpha_opt)\n",
    "\n",
    "best_RMSE_val = RMSE_found.min()\n",
    "best_alpha = alpha_found[RMSE_found == RMSE_found.min()][0]\n",
    "print(f'\\nThe best RMSE of the VAL set is {best_RMSE_val:.4f} while alpha is {best_alpha:.2f}.')\n",
    "\n",
    "# random initialisation\n",
    "d, k = X_train_pca.shape[1], 20\n",
    "ww = 0.1*np.random.randn(k)/np.sqrt(k)\n",
    "bb = 0.1*np.random.randn()/np.sqrt(k)\n",
    "V = 0.1*np.random.randn(k, d)/np.sqrt(k)\n",
    "bk = 0.1*np.random.randn(k)/np.sqrt(k)\n",
    "nn_init_random = (ww, bb, V, bk)\n",
    "\n",
    "# X_train, y_train and alpha\n",
    "nn_alpha = best_alpha\n",
    "nn_args = (X_train_pca, y_train, nn_alpha)\n",
    "\n",
    "nn_param_random_pca = minimize_list(nn_cost, nn_init_random, nn_args)\n",
    "\n",
    "pred_nn_random_train_pca = nn_cost(nn_param_random_pca, X=X_train_pca)\n",
    "RMSE_nn_random_train_pca = np.sqrt(np.mean((y_train - pred_nn_random_train_pca) ** 2))\n",
    "\n",
    "pred_nn_random_val_pca = nn_cost(nn_param_random_pca, X=X_val_pca)\n",
    "RMSE_nn_random_val_pca = np.sqrt(np.mean((y_val - pred_nn_random_val_pca) ** 2))\n",
    "\n",
    "pred_nn_random_test_pca = nn_cost(nn_param_random_pca, X=X_test_pca)\n",
    "RMSE_nn_random_test_pca = np.sqrt(np.mean((y_test - pred_nn_random_test_pca) ** 2))\n",
    "\n",
    "print(f'The alpha is {nn_alpha}.')\n",
    "print(f'The RMSE of nn with random initialisaiton using the TRAIN_pca set is {RMSE_nn_random_train_pca:.4f}.')\n",
    "print(f'The RMSE of nn with random initialisaiton using the VAL_pca set is {RMSE_nn_random_val_pca:.4f}.')\n",
    "print(f'The RMSE of nn with random initialisaiton using the TEST_pca set is {RMSE_nn_random_test_pca:.4f}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
